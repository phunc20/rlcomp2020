{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leif/.virtualenvs/rlcomp2020/lib64/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/leif/.virtualenvs/rlcomp2020/lib64/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/leif/.virtualenvs/rlcomp2020/lib64/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/leif/.virtualenvs/rlcomp2020/lib64/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/leif/.virtualenvs/rlcomp2020/lib64/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/leif/.virtualenvs/rlcomp2020/lib64/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/leif/.virtualenvs/rlcomp2020/lib64/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leif/.virtualenvs/rlcomp2020/lib64/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/leif/.virtualenvs/rlcomp2020/lib64/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/leif/.virtualenvs/rlcomp2020/lib64/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/leif/.virtualenvs/rlcomp2020/lib64/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/leif/.virtualenvs/rlcomp2020/lib64/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/leif/.virtualenvs/rlcomp2020/lib64/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "import datetime\n",
    "import json\n",
    "from array import *\n",
    "import os\n",
    "import math\n",
    "from random import randrange\n",
    "import random\n",
    "\n",
    "#from keras.models import Sequential\n",
    "#from keras.models import model_from_json\n",
    "#from keras.layers import Dense, Activation\n",
    "#from keras import optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.compat.v1.keras import backend as K\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function disable_v2_behavior in module tensorflow.python.compat.v2_compat:\n",
      "\n",
      "disable_v2_behavior()\n",
      "    Disables TensorFlow 2.x behaviors.\n",
      "    \n",
      "    This function can be called at the beginning of the program (before `Tensors`,\n",
      "    `Graphs` or other structures have been created, and before devices have been\n",
      "    initialized. It switches all global behaviors that are different between\n",
      "    TensorFlow 1.x and 2.x to behave as intended for 1.x.\n",
      "    \n",
      "    User can call this function to disable 2.x behavior during complex migrations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.disable_v2_behavior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classes in GAME_SOCKET_DUMMY.py\n",
    "class ObstacleInfo:\n",
    "    # initial energy for obstacles: Land (key = 0): -1, Forest(key = -1): 0 (random), Trap(key = -2): -10, Swamp (key = -3): -5\n",
    "    types = {0: -1, -1: 0, -2: -10, -3: -5}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.type = 0\n",
    "        self.posx = 0\n",
    "        self.posy = 0\n",
    "        self.value = 0\n",
    "        \n",
    "class GoldInfo:\n",
    "    def __init__(self):\n",
    "        self.posx = 0\n",
    "        self.posy = 0\n",
    "        self.amount = 0\n",
    "\n",
    "    def loads(self, data):\n",
    "        golds = []\n",
    "        for gd in data:\n",
    "            g = GoldInfo()\n",
    "            g.posx = gd[\"posx\"]\n",
    "            g.posy = gd[\"posy\"]\n",
    "            g.amount = gd[\"amount\"]\n",
    "            golds.append(g)\n",
    "        return golds\n",
    "\n",
    "class PlayerInfo:\n",
    "    STATUS_PLAYING = 0\n",
    "    STATUS_ELIMINATED_WENT_OUT_MAP = 1\n",
    "    STATUS_ELIMINATED_OUT_OF_ENERGY = 2\n",
    "    STATUS_ELIMINATED_INVALID_ACTION = 3\n",
    "    STATUS_STOP_EMPTY_GOLD = 4\n",
    "    STATUS_STOP_END_STEP = 5\n",
    "\n",
    "    def __init__(self, id):\n",
    "        self.playerId = id\n",
    "        self.score = 0\n",
    "        self.energy = 0\n",
    "        self.posx = 0\n",
    "        self.posy = 0\n",
    "        self.lastAction = -1\n",
    "        self.status = PlayerInfo.STATUS_PLAYING\n",
    "        self.freeCount = 0\n",
    "\n",
    "class GameInfo:\n",
    "    def __init__(self):\n",
    "        self.numberOfPlayers = 1\n",
    "        self.width = 0\n",
    "        self.height = 0\n",
    "        self.steps = 100\n",
    "        self.golds = []\n",
    "        self.obstacles = []\n",
    "\n",
    "    def loads(self, data):\n",
    "        m = GameInfo()\n",
    "        m.width = data[\"width\"]\n",
    "        m.height = data[\"height\"]\n",
    "        m.golds = GoldInfo().loads(data[\"golds\"])\n",
    "        m.obstacles = data[\"obstacles\"]\n",
    "        m.numberOfPlayers = data[\"numberOfPlayers\"]\n",
    "        m.steps = data[\"steps\"]\n",
    "        return m\n",
    "\n",
    "class UserMatch:\n",
    "    def __init__(self):\n",
    "        self.playerId = 1\n",
    "        self.posx = 0\n",
    "        self.posy = 0\n",
    "        self.energy = 50\n",
    "        self.gameinfo = GameInfo()\n",
    "\n",
    "    def to_json(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__, sort_keys=True, indent=4)\n",
    "\n",
    "class StepState:\n",
    "    def __init__(self):\n",
    "        self.players = []\n",
    "        self.golds = []\n",
    "        self.changedObstacles = []\n",
    "\n",
    "    def to_json(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__, sort_keys=True, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main class in GAME_SOCKET_DUMMY.py\n",
    "class GameSocket:\n",
    "    bog_energy_chain = {-5: -20, -20: -40, -40: -100, -100: -100}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stepCount = 0\n",
    "        self.maxStep = 0\n",
    "        self.mapdir = \"Maps\"  # where to load all pre-defined maps\n",
    "        self.mapid = \"\"\n",
    "        self.userMatch = UserMatch()\n",
    "        self.user = PlayerInfo(1)\n",
    "        self.stepState = StepState()\n",
    "        self.maps = {}  # key: map file name, value: file content\n",
    "        self.map = []  # running map info: 0->Land, -1->Forest, -2->Trap, -3:Swamp, >0:Gold\n",
    "        self.energyOnMap = []  # self.energyOnMap[x][y]: <0, amount of energy which player will consume if it move into (x,y)\n",
    "        self.E = 50\n",
    "        self.resetFlag = True\n",
    "        self.craftUsers = []  # players that craft at current step - for calculating amount of gold\n",
    "        self.bots = []\n",
    "        self.craftMap = {}  # cells that players craft at current step, key: x_y, value: number of players that craft at (x,y)\n",
    "\n",
    "    def init_bots(self):\n",
    "        self.bots = [Bot1(2), Bot2(3), Bot3(4)]  # use bot1(id=2), bot2(id=3), bot3(id=4)\n",
    "        #for (bot) in self.bots:  # at the beginning, all bots will have same position, energy as player\n",
    "        for bot in self.bots:  # at the beginning, all bots will have same position, energy as player\n",
    "            bot.info.posx = self.user.posx\n",
    "            bot.info.posy = self.user.posy\n",
    "            bot.info.energy = self.user.energy\n",
    "            bot.info.lastAction = -1\n",
    "            bot.info.status = PlayerInfo.STATUS_PLAYING\n",
    "            bot.info.score = 0\n",
    "            self.stepState.players.append(bot.info)\n",
    "        self.userMatch.gameinfo.numberOfPlayers = len(self.stepState.players)\n",
    "        #print(\"numberOfPlayers: \", self.userMatch.gameinfo.numberOfPlayers)\n",
    "\n",
    "    def reset(self, requests):  # load new game by given request: [map id (filename), posx, posy, initial energy]\n",
    "        # load new map\n",
    "        self.reset_map(requests[0])\n",
    "        self.userMatch.posx = int(requests[1])\n",
    "        self.userMatch.posy = int(requests[2])\n",
    "        self.userMatch.energy = int(requests[3])\n",
    "        self.userMatch.gameinfo.steps = int(requests[4])\n",
    "        self.maxStep = self.userMatch.gameinfo.steps\n",
    "\n",
    "        # init data for players\n",
    "        self.user.posx = self.userMatch.posx  # in\n",
    "        self.user.posy = self.userMatch.posy\n",
    "        self.user.energy = self.userMatch.energy\n",
    "        self.user.status = PlayerInfo.STATUS_PLAYING\n",
    "        self.user.score = 0\n",
    "        self.stepState.players = [self.user]\n",
    "        self.E = self.userMatch.energy\n",
    "        self.resetFlag = True\n",
    "        self.init_bots()\n",
    "        self.stepCount = 0\n",
    "\n",
    "    def reset_map(self, id):  # load map info\n",
    "        self.mapId = id\n",
    "        self.map = json.loads(self.maps[self.mapId])\n",
    "        self.userMatch = self.map_info(self.map)\n",
    "        self.stepState.golds = self.userMatch.gameinfo.golds\n",
    "        self.map = json.loads(self.maps[self.mapId])\n",
    "        self.energyOnMap = json.loads(self.maps[self.mapId])\n",
    "        for x in range(len(self.map)):\n",
    "            for y in range(len(self.map[x])):\n",
    "                if self.map[x][y] > 0:  # gold\n",
    "                    self.energyOnMap[x][y] = -4\n",
    "                else:  # obstacles\n",
    "                    self.energyOnMap[x][y] = ObstacleInfo.types[self.map[x][y]]\n",
    "\n",
    "    def connect(self): # simulate player's connect request\n",
    "        print(\"Connected to server.\")\n",
    "        for mapid in range(len(Maps)):\n",
    "            filename = \"map\" + str(mapid)\n",
    "            print(\"Found: \" + filename)\n",
    "            self.maps[filename] = str(Maps[mapid])\n",
    "\n",
    "    def map_info(self, map):  # get map info\n",
    "        # print(map)\n",
    "        userMatch = UserMatch()\n",
    "        userMatch.gameinfo.height = len(map)\n",
    "        userMatch.gameinfo.width = len(map[0])\n",
    "        i = 0\n",
    "        while i < len(map):\n",
    "            j = 0\n",
    "            while j < len(map[i]):\n",
    "                if map[i][j] > 0:  # gold\n",
    "                    g = GoldInfo()\n",
    "                    g.posx = j\n",
    "                    g.posy = i\n",
    "                    g.amount = map[i][j]\n",
    "                    userMatch.gameinfo.golds.append(g)\n",
    "                else:  # obstacles\n",
    "                    o = ObstacleInfo()\n",
    "                    o.posx = j\n",
    "                    o.posy = i\n",
    "                    o.type = -map[i][j]\n",
    "                    o.value = ObstacleInfo.types[map[i][j]]\n",
    "                    userMatch.gameinfo.obstacles.append(o)\n",
    "                j += 1\n",
    "            i += 1\n",
    "        return userMatch\n",
    "\n",
    "    def receive(self):  # send data to player (simulate player's receive request)\n",
    "        if self.resetFlag:  # for the first time -> send game info\n",
    "            self.resetFlag = False\n",
    "            data = self.userMatch.to_json()\n",
    "            for (bot) in self.bots:\n",
    "                bot.new_game(data)\n",
    "            # print(data)\n",
    "            return data\n",
    "        else:  # send step state\n",
    "            self.stepCount = self.stepCount + 1\n",
    "            if self.stepCount >= self.maxStep:\n",
    "                for player in self.stepState.players:\n",
    "                    player.status = PlayerInfo.STATUS_STOP_END_STEP\n",
    "            data = self.stepState.to_json()\n",
    "            #for (bot) in self.bots:  # update bots' state\n",
    "            for bot in self.bots:  # update bots' state\n",
    "                bot.new_state(data)\n",
    "            # print(data)\n",
    "            return data\n",
    "\n",
    "    def send(self, message):  # receive message from player (simulate send request from player)\n",
    "        if message.isnumeric():  # player send action\n",
    "            self.resetFlag = False\n",
    "            self.stepState.changedObstacles = []\n",
    "            action = int(message)\n",
    "            # print(\"Action = \", action)\n",
    "            self.user.lastAction = action\n",
    "            self.craftUsers = []\n",
    "            self.step_action(self.user, action)\n",
    "            for bot in self.bots:\n",
    "                if bot.info.status == PlayerInfo.STATUS_PLAYING:\n",
    "                    action = bot.next_action()\n",
    "                    bot.info.lastAction = action\n",
    "                    # print(\"Bot Action: \", action)\n",
    "                    self.step_action(bot.info, action)\n",
    "            self.action_5_craft()\n",
    "            for c in self.stepState.changedObstacles:\n",
    "                self.map[c[\"posy\"]][c[\"posx\"]] = -c[\"type\"]\n",
    "                self.energyOnMap[c[\"posy\"]][c[\"posx\"]] = c[\"value\"]\n",
    "\n",
    "        else:  # reset game\n",
    "            requests = message.split(\",\")\n",
    "            #print(\"Reset game: \", requests, end='')\n",
    "            self.reset(requests)\n",
    "\n",
    "    def step_action(self, user, action):\n",
    "        switcher = {\n",
    "            0: self.action_0_left,\n",
    "            1: self.action_1_right,\n",
    "            2: self.action_2_up,\n",
    "            3: self.action_3_down,\n",
    "            4: self.action_4_free,\n",
    "            5: self.action_5_craft_pre\n",
    "        }\n",
    "        func = switcher.get(action, self.invalidAction)\n",
    "        func(user)\n",
    "\n",
    "    def action_5_craft_pre(self, user):  # collect players who craft at current step\n",
    "        user.freeCount = 0\n",
    "        if self.map[user.posy][user.posx] <= 0:  # craft at the non-gold cell\n",
    "            user.energy -= 10\n",
    "            if user.energy <= 0:\n",
    "                user.status = PlayerInfo.STATUS_ELIMINATED_OUT_OF_ENERGY\n",
    "                user.lastAction = 6 #eliminated\n",
    "        else:\n",
    "            user.energy -= 5\n",
    "            if user.energy > 0:\n",
    "                self.craftUsers.append(user)\n",
    "                key = str(user.posx) + \"_\" + str(user.posy)\n",
    "                if key in self.craftMap:\n",
    "                    count = self.craftMap[key]\n",
    "                    self.craftMap[key] = count + 1\n",
    "                else:\n",
    "                    self.craftMap[key] = 1\n",
    "            else:\n",
    "                user.status = PlayerInfo.STATUS_ELIMINATED_OUT_OF_ENERGY\n",
    "                user.lastAction = 6 #eliminated\n",
    "\n",
    "    def action_0_left(self, user):  # user go left\n",
    "        user.freeCount = 0\n",
    "        user.posx = user.posx - 1\n",
    "        if user.posx < 0:\n",
    "            user.status = PlayerInfo.STATUS_ELIMINATED_WENT_OUT_MAP\n",
    "            user.lastAction = 6 #eliminated\n",
    "        else:\n",
    "            self.go_to_pos(user)\n",
    "\n",
    "    def action_1_right(self, user):  # user go right\n",
    "        user.freeCount = 0\n",
    "        user.posx = user.posx + 1\n",
    "        if user.posx >= self.userMatch.gameinfo.width:\n",
    "            user.status = PlayerInfo.STATUS_ELIMINATED_WENT_OUT_MAP\n",
    "            user.lastAction = 6 #eliminated\n",
    "        else:\n",
    "            self.go_to_pos(user)\n",
    "\n",
    "    def action_2_up(self, user):  # user go up\n",
    "        user.freeCount = 0\n",
    "        user.posy = user.posy - 1\n",
    "        if user.posy < 0:\n",
    "            user.status = PlayerInfo.STATUS_ELIMINATED_WENT_OUT_MAP\n",
    "            user.lastAction = 6 #eliminated\n",
    "        else:\n",
    "            self.go_to_pos(user)\n",
    "\n",
    "    def action_3_down(self, user):  # user go right\n",
    "        user.freeCount = 0\n",
    "        user.posy = user.posy + 1\n",
    "        if user.posy >= self.userMatch.gameinfo.height:\n",
    "            user.status = PlayerInfo.STATUS_ELIMINATED_WENT_OUT_MAP\n",
    "            user.lastAction = 6 #eliminated\n",
    "        else:\n",
    "            self.go_to_pos(user)\n",
    "\n",
    "    def action_4_free(self, user):  # user free\n",
    "        user.freeCount += 1\n",
    "        if user.freeCount == 1:\n",
    "            user.energy += int(self.E / 4)\n",
    "        elif user.freeCount == 2:\n",
    "            user.energy += int(self.E / 3)\n",
    "        elif user.freeCount == 3:\n",
    "            user.energy += int(self.E / 2)\n",
    "        else:\n",
    "            user.energy = self.E\n",
    "        if user.energy > self.E:\n",
    "            user.energy = self.E\n",
    "\n",
    "    def action_5_craft(self):\n",
    "        craftCount = len(self.craftUsers)\n",
    "        # print (\"craftCount\",craftCount)\n",
    "        if (craftCount > 0):\n",
    "            for user in self.craftUsers:\n",
    "                x = user.posx\n",
    "                y = user.posy\n",
    "                key = str(user.posx) + \"_\" + str(user.posy)\n",
    "                c = self.craftMap[key]\n",
    "                m = min(math.ceil(self.map[y][x] / c), 50)\n",
    "                user.score += m\n",
    "                # print (\"user\", user.playerId, m)\n",
    "            for user in self.craftUsers:\n",
    "                x = user.posx\n",
    "                y = user.posy\n",
    "                key = str(user.posx) + \"_\" + str(user.posy)\n",
    "                if key in self.craftMap:\n",
    "                    c = self.craftMap[key]\n",
    "                    del self.craftMap[key]\n",
    "                    m = min(math.ceil(self.map[y][x] / c), 50)\n",
    "                    self.map[y][x] -= m * c\n",
    "                    if self.map[y][x] < 0:\n",
    "                        self.map[y][x] = 0\n",
    "                        self.energyOnMap[y][x] = ObstacleInfo.types[0]\n",
    "                    for g in self.stepState.golds:\n",
    "                        if g.posx == x and g.posy == y:\n",
    "                            g.amount = self.map[y][x]\n",
    "                            if g.amount == 0:\n",
    "                                self.stepState.golds.remove(g)\n",
    "                                self.add_changed_obstacle(x, y, 0, ObstacleInfo.types[0])\n",
    "                                if len(self.stepState.golds) == 0:\n",
    "                                    for player in self.stepState.players:\n",
    "                                        player.status = PlayerInfo.STATUS_STOP_EMPTY_GOLD\n",
    "                            break;\n",
    "            self.craftMap = {}\n",
    "\n",
    "    def invalidAction(self, user):\n",
    "        user.status = PlayerInfo.STATUS_ELIMINATED_INVALID_ACTION\n",
    "        user.lastAction = 6 #eliminated\n",
    "\n",
    "    def go_to_pos(self, user):  # player move to cell(x,y)\n",
    "        if self.map[user.posy][user.posx] == -1:\n",
    "            user.energy -= randrange(16) + 5\n",
    "        elif self.map[user.posy][user.posx] == 0:\n",
    "            user.energy += self.energyOnMap[user.posy][user.posx]\n",
    "        elif self.map[user.posy][user.posx] == -2:\n",
    "            user.energy += self.energyOnMap[user.posy][user.posx]\n",
    "            self.add_changed_obstacle(user.posx, user.posy, 0, ObstacleInfo.types[0])\n",
    "        elif self.map[user.posy][user.posx] == -3:\n",
    "            user.energy += self.energyOnMap[user.posy][user.posx]\n",
    "            self.add_changed_obstacle(user.posx, user.posy, 3,\n",
    "                                      self.bog_energy_chain[self.energyOnMap[user.posy][user.posx]])\n",
    "        else:\n",
    "            user.energy -= 4\n",
    "        if user.energy <= 0:\n",
    "            user.status = PlayerInfo.STATUS_ELIMINATED_OUT_OF_ENERGY\n",
    "            user.lastAction = 6 #eliminated\n",
    "\n",
    "    def add_changed_obstacle(self, x, y, t, v):\n",
    "        added = False\n",
    "        for o in self.stepState.changedObstacles:\n",
    "            if o[\"posx\"] == x and o[\"posy\"] == y:\n",
    "                added = True\n",
    "                break\n",
    "        if added == False:\n",
    "            o = {}\n",
    "            o[\"posx\"] = x\n",
    "            o[\"posy\"] = y\n",
    "            o[\"type\"] = t\n",
    "            o[\"value\"] = v\n",
    "            self.stepState.changedObstacles.append(o)\n",
    "\n",
    "    def close(self):\n",
    "        print(\"Close socket.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bots :bot1\n",
    "class Bot1:\n",
    "    ACTION_GO_LEFT = 0\n",
    "    ACTION_GO_RIGHT = 1\n",
    "    ACTION_GO_UP = 2\n",
    "    ACTION_GO_DOWN = 3\n",
    "    ACTION_FREE = 4\n",
    "    ACTION_CRAFT = 5\n",
    "\n",
    "    def __init__(self, id):\n",
    "        self.state = State()\n",
    "        self.info = PlayerInfo(id)\n",
    "\n",
    "    def next_action(self):\n",
    "        if self.state.mapInfo.gold_amount(self.info.posx, self.info.posy) > 0:\n",
    "            if self.info.energy >= 6:\n",
    "                return self.ACTION_CRAFT\n",
    "            else:\n",
    "                return self.ACTION_FREE\n",
    "        if self.info.energy < 5:\n",
    "            return self.ACTION_FREE\n",
    "        else:\n",
    "            action = self.ACTION_GO_UP\n",
    "            if self.info.posy % 2 == 0:\n",
    "                if self.info.posx < self.state.mapInfo.max_x:\n",
    "                    action = self.ACTION_GO_RIGHT\n",
    "            else:\n",
    "                if self.info.posx > 0:\n",
    "                    action = self.ACTION_GO_LEFT\n",
    "                else:\n",
    "                    action = self.ACTION_GO_DOWN\n",
    "            return action\n",
    "\n",
    "    def new_game(self, data):\n",
    "        try:\n",
    "            self.state.init_state(data)\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "    def new_state(self, data):\n",
    "        # action = self.next_action();\n",
    "        # self.socket.send(action)\n",
    "        try:\n",
    "            self.state.update_state(data)\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bots :bot2\n",
    "class Bot2:\n",
    "    ACTION_GO_LEFT = 0\n",
    "    ACTION_GO_RIGHT = 1\n",
    "    ACTION_GO_UP = 2\n",
    "    ACTION_GO_DOWN = 3\n",
    "    ACTION_FREE = 4\n",
    "    ACTION_CRAFT = 5\n",
    "\n",
    "    def __init__(self, id):\n",
    "        self.state = State()\n",
    "        self.info = PlayerInfo(id)\n",
    "\n",
    "    def next_action(self):\n",
    "        if self.state.mapInfo.gold_amount(self.info.posx, self.info.posy) > 0:\n",
    "            if self.info.energy >= 6:\n",
    "                return self.ACTION_CRAFT\n",
    "            else:\n",
    "                return self.ACTION_FREE\n",
    "        if self.info.energy < 5:\n",
    "            return self.ACTION_FREE\n",
    "        else:\n",
    "            action = np.random.randint(0, 4)            \n",
    "            return action\n",
    "\n",
    "    def new_game(self, data):\n",
    "        try:\n",
    "            self.state.init_state(data)\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "    def new_state(self, data):\n",
    "        # action = self.next_action();\n",
    "        # self.socket.send(action)\n",
    "        try:\n",
    "            self.state.update_state(data)\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bots :bot3\n",
    "class Bot3:\n",
    "    ACTION_GO_LEFT = 0\n",
    "    ACTION_GO_RIGHT = 1\n",
    "    ACTION_GO_UP = 2\n",
    "    ACTION_GO_DOWN = 3\n",
    "    ACTION_FREE = 4\n",
    "    ACTION_CRAFT = 5\n",
    "\n",
    "    def __init__(self, id):\n",
    "        self.state = State()\n",
    "        self.info = PlayerInfo(id)\n",
    "\n",
    "    def next_action(self):\n",
    "        if self.state.mapInfo.gold_amount(self.info.posx, self.info.posy) > 0:\n",
    "            if self.info.energy >= 6:\n",
    "                return self.ACTION_CRAFT\n",
    "            else:\n",
    "                return self.ACTION_FREE\n",
    "        if self.info.energy < 5:\n",
    "            return self.ACTION_FREE\n",
    "        else:\n",
    "            action = self.ACTION_GO_LEFT\n",
    "            if self.info.posx % 2 == 0:\n",
    "                if self.info.posy < self.state.mapInfo.max_y:\n",
    "                    action = self.ACTION_GO_DOWN\n",
    "            else:\n",
    "                if self.info.posy > 0:\n",
    "                    action = self.ACTION_GO_UP\n",
    "                else:\n",
    "                    action = self.ACTION_GO_RIGHT            \n",
    "            return action\n",
    "\n",
    "    def new_game(self, data):\n",
    "        try:\n",
    "            self.state.init_state(data)\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "    def new_state(self, data):\n",
    "        # action = self.next_action();\n",
    "        # self.socket.send(action)\n",
    "        try:\n",
    "            self.state.update_state(data)\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MinerState.py\n",
    "def str_2_json(str):\n",
    "    return json.loads(str, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "class MapInfo:\n",
    "    def __init__(self):\n",
    "        self.max_x = 0 #Width of the map\n",
    "        self.max_y = 0 #Height of the map\n",
    "        self.golds = [] #List of the golds in the map\n",
    "        self.obstacles = []\n",
    "        self.numberOfPlayers = 0\n",
    "        self.maxStep = 0 #The maximum number of step is set for this map\n",
    "\n",
    "    def init_map(self, gameInfo):\n",
    "        #Initialize the map at the begining of each episode\n",
    "        self.max_x = gameInfo[\"width\"] - 1\n",
    "        self.max_y = gameInfo[\"height\"] - 1\n",
    "        self.golds = gameInfo[\"golds\"]\n",
    "        self.obstacles = gameInfo[\"obstacles\"]\n",
    "        self.maxStep = gameInfo[\"steps\"]\n",
    "        self.numberOfPlayers = gameInfo[\"numberOfPlayers\"]\n",
    "\n",
    "    def update(self, golds, changedObstacles):\n",
    "        #Update the map after every step\n",
    "        self.golds = golds\n",
    "        for cob in changedObstacles:\n",
    "            newOb = True\n",
    "            for ob in self.obstacles:\n",
    "                if cob[\"posx\"] == ob[\"posx\"] and cob[\"posy\"] == ob[\"posy\"]:\n",
    "                    newOb = False\n",
    "                    #print(\"cell(\", cob[\"posx\"], \",\", cob[\"posy\"], \") change type from: \", ob[\"type\"], \" -> \",\n",
    "                    #      cob[\"type\"], \" / value: \", ob[\"value\"], \" -> \", cob[\"value\"])\n",
    "                    ob[\"type\"] = cob[\"type\"]\n",
    "                    ob[\"value\"] = cob[\"value\"]\n",
    "                    break\n",
    "            if newOb:\n",
    "                self.obstacles.append(cob)\n",
    "                #print(\"new obstacle: \", cob[\"posx\"], \",\", cob[\"posy\"], \", type = \", cob[\"type\"], \", value = \",\n",
    "                #      cob[\"value\"])\n",
    "\n",
    "    def get_min_x(self):\n",
    "        return min([cell[\"posx\"] for cell in self.golds])\n",
    "\n",
    "    def get_max_x(self):\n",
    "        return max([cell[\"posx\"] for cell in self.golds])\n",
    "\n",
    "    def get_min_y(self):\n",
    "        return min([cell[\"posy\"] for cell in self.golds])\n",
    "\n",
    "    def get_max_y(self):\n",
    "        return max([cell[\"posy\"] for cell in self.golds])\n",
    "\n",
    "    def is_row_has_gold(self, y):\n",
    "        return y in [cell[\"posy\"] for cell in self.golds]\n",
    "\n",
    "    def is_column_has_gold(self, x):\n",
    "        return x in [cell[\"posx\"] for cell in self.golds]\n",
    "\n",
    "    def gold_amount(self, x, y): #Get the amount of golds at cell (x,y)\n",
    "        for cell in self.golds:\n",
    "            if x == cell[\"posx\"] and y == cell[\"posy\"]:\n",
    "                return cell[\"amount\"]\n",
    "        return 0 \n",
    "\n",
    "    def get_obstacle(self, x, y):  # Get the kind of the obstacle at cell(x,y)\n",
    "        for cell in self.obstacles:\n",
    "            if x == cell[\"posx\"] and y == cell[\"posy\"]:\n",
    "                return cell[\"type\"]\n",
    "        return -1  # No obstacle at the cell (x,y)\n",
    "\n",
    "\n",
    "class State:\n",
    "    STATUS_PLAYING = 0\n",
    "    STATUS_ELIMINATED_WENT_OUT_MAP = 1\n",
    "    STATUS_ELIMINATED_OUT_OF_ENERGY = 2\n",
    "    STATUS_ELIMINATED_INVALID_ACTION = 3\n",
    "    STATUS_STOP_EMPTY_GOLD = 4\n",
    "    STATUS_STOP_END_STEP = 5\n",
    "\n",
    "    def __init__(self):\n",
    "        self.end = False\n",
    "        self.score = 0\n",
    "        self.lastAction = None\n",
    "        self.id = 0\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        self.energy = 0\n",
    "        self.mapInfo = MapInfo()\n",
    "        self.players = []\n",
    "        self.stepCount = 0\n",
    "        self.status = State.STATUS_PLAYING\n",
    "\n",
    "    def init_state(self, data): #parse data from server into object\n",
    "        game_info = str_2_json(data)\n",
    "        self.end = False\n",
    "        self.score = 0\n",
    "        self.lastAction = None\n",
    "        self.id = game_info[\"playerId\"]\n",
    "        self.x = game_info[\"posx\"]\n",
    "        self.y = game_info[\"posy\"]\n",
    "        self.energy = game_info[\"energy\"]\n",
    "        self.mapInfo.init_map(game_info[\"gameinfo\"])\n",
    "        self.stepCount = 0\n",
    "        self.status = State.STATUS_PLAYING\n",
    "        self.players = [{\"playerId\": 2, \"posx\": self.x, \"posy\": self.y},\n",
    "                        {\"playerId\": 3, \"posx\": self.x, \"posy\": self.y},\n",
    "                        {\"playerId\": 4, \"posx\": self.x, \"posy\": self.y}]\n",
    "\n",
    "    def update_state(self, data):\n",
    "        new_state = str_2_json(data)\n",
    "        for player in new_state[\"players\"]:\n",
    "            if player[\"playerId\"] == self.id:\n",
    "                self.x = player[\"posx\"]\n",
    "                self.y = player[\"posy\"]\n",
    "                self.energy = player[\"energy\"]\n",
    "                self.score = player[\"score\"]\n",
    "                self.lastAction = player[\"lastAction\"]\n",
    "                self.status = player[\"status\"]\n",
    "\n",
    "        self.mapInfo.update(new_state[\"golds\"], new_state[\"changedObstacles\"])\n",
    "        self.players = new_state[\"players\"]\n",
    "        for i in range(len(self.players), 4, 1):\n",
    "            self.players.append({\"playerId\": i, \"posx\": self.x, \"posy\": self.y})\n",
    "        self.stepCount = self.stepCount + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MinerEnv.py\n",
    "TreeID = 1\n",
    "TrapID = 2\n",
    "SwampID = 3\n",
    "class MinerEnv:\n",
    "    def __init__(self):\n",
    "        self.socket = GameSocket()\n",
    "        self.state = State()\n",
    "        \n",
    "        self.score_pre = self.state.score#Storing the last score for designing the reward function\n",
    "\n",
    "    def start(self): #connect to server\n",
    "        self.socket.connect()\n",
    "\n",
    "    def end(self): #disconnect server\n",
    "        self.socket.close()\n",
    "\n",
    "    def send_map_info(self, request):#tell server which map to run\n",
    "        self.socket.send(request)\n",
    "\n",
    "    def reset(self): #start new game\n",
    "        try:\n",
    "            message = self.socket.receive() #receive game info from server\n",
    "            self.state.init_state(message) #init state\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "    def step(self, action): #step process\n",
    "        self.socket.send(action) #send action to server\n",
    "        try:\n",
    "            message = self.socket.receive() #receive new state from server\n",
    "            self.state.update_state(message) #update to local state\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "    # Functions are customized by client\n",
    "    def get_state(self):\n",
    "        # Building the map\n",
    "        view = np.zeros([self.state.mapInfo.max_x + 1, self.state.mapInfo.max_y + 1], dtype=int)\n",
    "        for i in range(self.state.mapInfo.max_x + 1):\n",
    "            for j in range(self.state.mapInfo.max_y + 1):\n",
    "                if self.state.mapInfo.get_obstacle(i, j) == TreeID:  # Tree\n",
    "                    view[i, j] = -TreeID\n",
    "                if self.state.mapInfo.get_obstacle(i, j) == TrapID:  # Trap\n",
    "                    view[i, j] = -TrapID\n",
    "                if self.state.mapInfo.get_obstacle(i, j) == SwampID: # Swamp\n",
    "                    view[i, j] = -SwampID\n",
    "                if self.state.mapInfo.gold_amount(i, j) > 0:\n",
    "                    view[i, j] = self.state.mapInfo.gold_amount(i, j)\n",
    "\n",
    "        DQNState = view.flatten().tolist() #Flattening the map matrix to a vector\n",
    "        \n",
    "        # Add position and energy of agent to the DQNState\n",
    "        DQNState.append(self.state.x)\n",
    "        DQNState.append(self.state.y)\n",
    "        DQNState.append(self.state.energy)\n",
    "        #Add position of bots \n",
    "        for player in self.state.players:\n",
    "            if player[\"playerId\"] != self.state.id:\n",
    "                DQNState.append(player[\"posx\"])\n",
    "                DQNState.append(player[\"posy\"])\n",
    "                \n",
    "        #Convert the DQNState from list to array for training\n",
    "        DQNState = np.array(DQNState)\n",
    "\n",
    "        return DQNState\n",
    "\n",
    "    def get_reward(self):\n",
    "        # Calculate reward\n",
    "        reward = 0\n",
    "        score_action = self.state.score - self.score_pre\n",
    "        self.score_pre = self.state.score\n",
    "        if score_action > 0:\n",
    "            #If the DQN agent crafts golds, then it should obtain a positive reward (equal score_action)\n",
    "            reward += score_action\n",
    "            \n",
    "        #If the DQN agent crashs into obstacels (Tree, Trap, Swamp), then it should be punished by a negative reward\n",
    "        if self.state.mapInfo.get_obstacle(self.state.x, self.state.y) == TreeID:  # Tree\n",
    "            reward -= TreeID\n",
    "        if self.state.mapInfo.get_obstacle(self.state.x, self.state.y) == TrapID:  # Trap\n",
    "            reward -= TrapID\n",
    "        if self.state.mapInfo.get_obstacle(self.state.x, self.state.y) == SwampID:  # Swamp\n",
    "            reward -= SwampID\n",
    "\n",
    "        # If out of the map, then the DQN agent should be punished by a larger nagative reward.\n",
    "        if self.state.status == State.STATUS_ELIMINATED_WENT_OUT_MAP:\n",
    "            reward += -10\n",
    "            \n",
    "        #Run out of energy, then the DQN agent should be punished by a larger nagative reward.\n",
    "        if self.state.status == State.STATUS_ELIMINATED_OUT_OF_ENERGY:\n",
    "            reward += -10\n",
    "        # print (\"reward\",reward)\n",
    "        return reward\n",
    "\n",
    "    def check_terminate(self):\n",
    "        #Checking the status of the game\n",
    "        #it indicates the game ends or is playing\n",
    "        return self.state.status != State.STATUS_PLAYING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the target model initialization a bit."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#DQNModel.py\n",
    "class DQN: \n",
    "    def __init__(self,\n",
    "                 input_dim, #The number of inputs for the DQN network\n",
    "                 action_space, #The number of actions for the DQN network\n",
    "                 gamma = 0.99, #The discount factor\n",
    "                 epsilon = 1, #Epsilon - the exploration factor\n",
    "                 epsilon_min = 0.01, #The minimum epsilon \n",
    "                 epsilon_decay = 0.999,#The decay epislon for each update_epsilon time\n",
    "                 learning_rate = 0.00025, #The learning rate for the DQN network\n",
    "                 tau = 0.125, #The factor for updating the DQN target network from the DQN network\n",
    "                 model = None, #The DQN model\n",
    "                 target_model = None, #The DQN target model \n",
    "                 sess=None):\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.action_space = action_space\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "              \n",
    "        self.model        = self.create_model() \n",
    "        #self.target_model = self.create_model()\n",
    "        self.target_model = keras.models.clone_model(self.model)\n",
    "        \n",
    "        #Tensorflow GPU optimization\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=config)\n",
    "        K.set_session(sess)\n",
    "        self.sess.run(tf.global_variables_initializer()) \n",
    "      \n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(300, input_dim=self.input_dim))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(300))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(self.action_space))\n",
    "        # (?1) \"linear\" activation, isn't it equiv. to no activation at all?\n",
    "        model.add(Activation('linear'))    \n",
    "        #adam = optimizers.adam(lr=self.learning_rate)\n",
    "        sgd = optimizers.SGD(lr=self.learning_rate, decay=1e-6, momentum=0.95)\n",
    "        model.compile(optimizer=sgd, loss='mse')\n",
    "        return model\n",
    "  \n",
    "    \n",
    "    def act(self, state):\n",
    "        #Get the index of the maximum Q values      \n",
    "        a_max = np.argmax(self.model.predict(state.reshape(1,len(state))))      \n",
    "        if (random.random() < self.epsilon):\n",
    "            # Recall: random.random() is a uniform r.v. in [0,1)\n",
    "            a_chosen = randrange(self.action_space)\n",
    "        else:\n",
    "            a_chosen = a_max      \n",
    "        return a_chosen\n",
    "    \n",
    "    \n",
    "    def replay(self, samples, batch_size):\n",
    "        inputs = np.zeros((batch_size, self.input_dim))\n",
    "        targets = np.zeros((batch_size, self.action_space))\n",
    "        \n",
    "        #for i in range(0, batch_size):\n",
    "        for i in range(batch_size):\n",
    "            state = samples[0][i,:]\n",
    "            action = samples[1][i]\n",
    "            reward = samples[2][i]\n",
    "            new_state = samples[3][i,:]\n",
    "            done = samples[4][i]\n",
    "            \n",
    "            inputs[i,:] = state\n",
    "            targets[i,:] = self.target_model.predict(state.reshape(1,len(state)))        \n",
    "            if done:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                Q_future = np.max(self.target_model.predict(new_state.reshape(1,len(new_state))))\n",
    "                targets[i, action] = reward + Q_future * self.gamma\n",
    "        #Training\n",
    "        loss = self.model.train_on_batch(inputs, targets)  \n",
    "\n",
    "    def target_train(self): \n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        #for i in range(0, len(target_weights)):\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = weights[i] * self.tau + target_weights[i] * (1 - self.tau)\n",
    "\n",
    "        self.target_model.set_weights(target_weights) \n",
    "\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        self.epsilon =  self.epsilon*self.epsilon_decay\n",
    "        self.epsilon =  max(self.epsilon_min, self.epsilon)\n",
    "\n",
    "\n",
    "    def save_model(self, model_name):\n",
    "        # serialize model to JSON\n",
    "        model_json = self.model.to_json()\n",
    "        with open(model_name + \".json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "            # serialize weights to HDF5\n",
    "            self.model.save_weights(model_name + \".h5\")\n",
    "            print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Memory.py\n",
    "class Memory:        \n",
    "    capacity = None  \n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            capacity,\n",
    "            length = None,\n",
    "            states = None,\n",
    "            actions = None,\n",
    "            rewards = None,\n",
    "            dones = None,\n",
    "            states2 = None,       \n",
    "    ):\n",
    "        self.capacity = capacity\n",
    "        self.length = 0\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "        self.dones = dones\n",
    "        self.states2 = states2\n",
    "\n",
    "    def push(self, s, a, r, done, s2):\n",
    "        if self.states is None:\n",
    "            self.states = s\n",
    "            self.actions = a\n",
    "            self.rewards = r\n",
    "            self.dones = done\n",
    "            self.states2 = s2\n",
    "        else:\n",
    "            self.states = np.vstack((self.states,s))\n",
    "            self.actions = np.vstack((self.actions,a))\n",
    "            self.rewards = np.vstack((self.rewards, r))\n",
    "            self.dones = np.vstack((self.dones, done))\n",
    "            self.states2 = np.vstack((self.states2,s2))\n",
    "        \n",
    "        self.length = self.length + 1\n",
    "            \n",
    "        if (self.length > self.capacity): \n",
    "            self.states = np.delete(self.states,(0), axis = 0)\n",
    "            self.actions = np.delete(self.actions,(0), axis = 0)\n",
    "            self.rewards = np.delete(self.rewards,(0), axis = 0)\n",
    "            self.dones = np.delete(self.dones,(0), axis = 0)\n",
    "            self.states2 = np.delete(self.states2,(0), axis = 0)           \n",
    "            self.length = self.length - 1\n",
    "            \n",
    "        \n",
    "    def sample(self,batch_size):\n",
    "        if (self.length >= batch_size):\n",
    "            idx = random.sample(range(0,self.length),batch_size)\n",
    "            s = self.states[idx,:]\n",
    "            a = self.actions[idx,:]\n",
    "            r = self.rewards[idx,:]\n",
    "            d = self.dones[idx,:]\n",
    "            s2 = self.states2[idx,:]\n",
    "                \n",
    "            return list([s,a,r,s2,d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Maps\n",
    "#This function is used to create 05 maps instead of loading them from Maps folder in the local\n",
    "def CreateMaps():\n",
    "    map1 = [\n",
    "      [0,  0,  -2,  100,  0,  0,  -1,  -1,  -3,  0,  0,  0,  -1,  -1,  0,  0,  -3,  0,  -1,  -1,0],\n",
    "      [-1,-1,  -2,  0, 0,  0,  -3,  -1,  0,  -2,  0,  0,  0,  -1,  0,  -1,  0,  -2,  -1,  0,0],\n",
    "      [0,  0,  -1,  0,  0,  0,  0,  -1,  -1,  -1,  0, 0,  100,  0,  0,  0,  0,  50,  -2,  0,0],\n",
    "      [0,  0,  0,  0,  -2,  0,  0,  0,  0,  0,  0,  0,  -1,  50, -2,  0,  0,  -1,  -1,  0,0],\n",
    "      [-2, 0,  200,  -2,  -2,  300,  0, 0,  -2,  -2,  0,  0,  -3,  0,  -1,  0,  0,  -3,  -1,  0,0],\n",
    "      [0,  -1,  0,  0,  0,  0,  0,  -3,  0,  0,  -1,  -1,  0,  0,  0,  0,  0,  0,  -2,  0,0],\n",
    "      [0,  -1,  -1,  0,  0,  -1,  -1,  0,  0,  700,  -1,  0,  0,  0,  -2,  -1,  -1,  0,  0, 0,100],\n",
    "      [0,  0, 0, 500,  0,  0,  -1,  0,  -2,  -2,  -1,  -1,  0,  0,  -2,  0,  -3,  0,  0,  -1,0],\n",
    "      [-1,  -1, 0,-2 ,  0,  -1,  -2,  0,  400,  -2,  -1,  -1,  500,  0,  -2,  0,  -3,  100,  0, 0,0]\n",
    "    ]\n",
    "    map2 = [\n",
    "      [0,  0,  -2,  0,  0,  0,  -1,  -1,  -3,  0,  0,  0,  -1,  -1,  0,  0,  -3,  0,  -1,  -1,0],\n",
    "      [-1,-1,  -2,  100, 0,  0,  -3,  -1,  0,  -2,  100,  0,  0,  -1,  0,  -1,  0,  -2,  -1,  0,0],\n",
    "      [0,  0,  -1,  0,  0,  0,  0,  -1,  -1,  -1,  0, 0,  0,  0,  0,  0,  50,  0,  -2,  0,0],\n",
    "      [0,  200,  0,  0,  -2,  0,  0,  0,  0,  0,  0,  0,  -1,  50, -2,  0,  0,  -1,  -1,  0,0],\n",
    "      [-2, 0,  0,  -2,  -2,  0,  0, 0,  -2,  -2,  0,  0,  -3,  0,  -1,  0,  0,  -3,  -1,  0,0],\n",
    "      [0,  -1,  0,  0,  300,  0,  0,  -3,  0,  0,  -1,  -1,  0,  0,  0,  0,  0,  0,  -2,  0,0],\n",
    "      [500,  -1,  -1,  0,  0,  -1,  -1,  0,  700,  0,  -1,  0,  0,  0,  -2,  -1,  -1,  0,  0, 0,0],\n",
    "      [0,  0, 0, 0,  0,  0,  -1,  0,  -2,  -2,  -1,  -1,  0,  0,  -2,  0,  -3,  100,  0,  -1,0],\n",
    "      [-1,  -1, 0,-2 ,  0,  -1,  -2,  400,  0,  -2,  -1,  -1,  0,  500,  -2,  0,  -3,  0,  0, 100,0]\n",
    "    ]\n",
    "    map3= [\n",
    "      [0,  0,  -2,  0,  0,  0,  -1,  -1,  -3,  0,  100,  0,  -1,  -1,  0,  0,  -3,  0,  -1,  -1,0],\n",
    "      [-1,-1,  -2,  0, 0,  0,  -3,  -1,  0,  -2,  0,  0,  0,  -1,  0,  -1,  0,  -2,  -1,  0,0 ],\n",
    "      [0,  0,  -1,  0,  0,  0,  100,  -1,  -1,  -1,  0, 0,  50,  0,  0,  0,  50,  0,  -2,  0,0],\n",
    "      [0,  200,  0,  0,  -2,  0,  0,  0,  0,  0,  0,  0,  -1,  0, -2,  0,  0,  -1,  -1,  0,0],\n",
    "      [-2, 0,  0,  -2,  -2,  0,  0, 0,  -2,  -2,  0,  0,  -3,  0,  -1,  0,  0,  -3,  -1,  0,0],\n",
    "      [0,  -1,  0, 300,  0,  0,  0,  -3,  0,  0,  -1,  -1,  0,  0,  0,  0,  0,  0,  -2,  0,0],\n",
    "      [0,  -1,  -1,  0,  0,  -1,  -1,  700,  0,  0,  -1,  0,  0,  0,  -2,  -1,  -1,  0,  0, 0,0],\n",
    "      [0,  0, 0, 0,  0,  500,  -1,  0,  -2,  -2,  -1,  -1,  0,  0,  -2,  0,  -3,  0,  700,  -1,0],\n",
    "      [-1,  -1, 0,-2 ,  0,  -1,  -2,  400,  0,  -2,  -1,  -1,  0,  500,  -2,  0,  -3,  0,  0, 100,0]\n",
    "    ]\n",
    "    map4=[\n",
    "      [0,  0,  -2,  0,  0,  0,  -1,  -1,  -3,  0,  0,  0,  -1,  -1,  0,  0,  -3,  0,  -1,  -1,0],\n",
    "      [-1,-1,  -2,  0, 0,  0,  -3,  -1,  0,  -2,  0,  0,  100,  -1,  0,  -1,  0,  -2,  -1,  0,0],\n",
    "      [0,  0,  -1,  0,  100,  0,  0,  -1,  -1,  -1,  0, 0,  0,  0,  50,  0,  50,  0,  -2,  0,0],\n",
    "      [0,  200,  0,  0,  -2,  0,  0,  0,  0,  0,  0,  0,  -1,  0, -2,  0,  0,  -1,  -1,  0,0],\n",
    "      [-2, 0,  0,  -2,  -2,  0,  0, 0,  -2,  -2,  0,  0,  -3,  0,  -1,  0,  0,  -3,  -1,  0,0],\n",
    "      [0,  -1,  0,  0,  0,  0,  300,  -3,  0,  700,  -1,  -1,  0,  0,  0,  0,  0,  0,  -2,  0,0],\n",
    "      [0,  -1,  -1,  0,  0,  -1,  -1,  0,  0,  0,  -1,  0,  0,  0,  -2,  -1,  -1,  0,  0, 100,0],\n",
    "      [500,  0, 0, 0,  0,  0,  -1,  0,  -2,  -2,  -1,  -1,  0,  0,  -2,  0,  -3,  0,  0,  -1,0],\n",
    "      [-1,  -1, 0,-2 ,  0,  -1,  -2,  400,  0,  -2,  -1,  -1,  0,  500,  -2,  0,  -3,  0,  0, 100,0]\n",
    "\n",
    "    ]\n",
    "    map5=[\n",
    "      [0,  0,  -2,  0,  100,  0,  -1,  -1,  -3,  0,  0,  0,  -1,  -1,  0,  0,  -3,  0,  -1,  -1,0],\n",
    "      [-1,-1,  -2,  0, 0,  0,  -3,  -1,  0,  -2,  100,  0,  0,  -1,  0,  -1,  0,  -2,  -1,  0,0],\n",
    "      [0,  0,  -1,  0,  0,  0,  0,  -1,  -1,  -1,  0, 0,  0,  0,  50,  0,  0,  0,  -2,  0,0],\n",
    "      [0,  200,  0,  0,  -2,  0,  0,  0,  0,  0,  0,  0,  -1,  0, -2,  0,  50,  -1,  -1,  0,0],\n",
    "      [-2, 0,  0,  -2,  -2,  0,  0, 0,  -2,  -2,  0,  0,  -3,  0,  -1,  0,  0,  -3,  -1,  0,0],\n",
    "      [0,  -1,  0,  0,  300,  0,  0,  -3,  0,  0,  -1,  -1,  0,  0,  0,  0,  0,  0,  -2,  0,0],\n",
    "      [500,  -1,  -1,  0,  0,  -1,  -1,  0,  0,  700,  -1,  0,  0,  0,  -2,  -1,  -1,  0,  0, 100,0],\n",
    "      [0,  0, 0, 0,  0,  0,  -1,  0,  -2,  -2,  -1,  -1,  0,  0,  -2,  0,  -3,  0,  0,  -1,0],\n",
    "      [-1,  -1, 0,-2 ,  0,  -1,  -2,  400,  0,  -2,  -1,  -1,  0,  500,  -2,  0,  -3,  0,  0, 100,0]\n",
    "    ]\n",
    "    Maps = (map1,map2,map3,map4,map5)\n",
    "    return Maps"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show that the training result of Phong's model under <code>10 000</code> episodes was <b>bad</b>, let's test with a dumb agent which does nothing but <b>stay put</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPISODES = 10_000\n",
    "MAX_STEP = 1000\n",
    "BATCH_SIZE = 32\n",
    "MEMORY_SIZE = 100_000 \n",
    "SAVE_NETWORK = 100  \n",
    "INITIAL_REPLAY_SIZE = 1000 \n",
    "INPUT_DIMS = 198\n",
    "N_ACTIONS = 6  \n",
    "MAP_MAX_X = 21 \n",
    "MAP_MAX_Y = 9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 300)               59700     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 1806      \n",
      "=================================================================\n",
      "Total params: 151,806\n",
      "Trainable params: 151,806\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "#tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(300, activation=\"relu\", input_shape=[INPUT_DIMS]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(N_ACTIONS, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
       "        0.16666667]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.zeros((1, INPUT_DIMS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.zeros((1, INPUT_DIMS))).sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "help(np.random.choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_proba = [[0.2, 0.1, 0.05, 0.6, 0, 0.05]]\n",
    "action = np.random.choice(range(6), p=action_proba[0])\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=127, shape=(1, 1), dtype=float32, numpy=array([[3.]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([[action]], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=129, shape=(3, 4), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(np.zeros((3,4)), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target = [[0. 0. 0. 1. 0. 0.]]\n",
      "y_target_pre = [[0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "N_ACTIONS = 6\n",
    "y_target_pre = np.zeros((1, N_ACTIONS), dtype=np.float32)\n",
    "y_target_pre[0, action] = 1.\n",
    "y_target = tf.constant(y_target_pre)\n",
    "print(\"y_target = {}\\ny_target_pre = {}\".format(y_target, y_target_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        action_proba = model(obs[np.newaxis])\n",
    "        #action = (tf.random.uniform([1, 1]) > left_proba)\n",
    "        action = np.random.choice(range(N_ACTIONS), p=action_proba[0])\n",
    "        #y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n",
    "        y_target_pre = np.zeros((1, N_ACTIONS), dtype=np.float32)\n",
    "        y_target_pre[0, action] = 1.\n",
    "        y_target = tf.constant(y_target_pre)\n",
    "        #y_target = tf.constant(y_target_pre, dtype=tf.float32)\n",
    "        # \"pretend for now that whatever action it takes is the right one\"\n",
    "        loss = tf.reduce_mean(loss_fn(y_target, action_proba))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    #obs, reward, done, info = env.step(int(action[0, 0].numpy()))\n",
    "    env.step(str(action))\n",
    "    obs = env.get_state()\n",
    "    reward = env.get_reward()\n",
    "    done = env.check_terminate()\n",
    "    return obs, reward, done, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_MAX_X = 21\n",
    "MAP_MAX_Y = 9\n",
    "\n",
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    all_n_steps = []\n",
    "    for episode in range(n_episodes):\n",
    "        mapID = np.random.randint(0, 5)\n",
    "        posID_x = np.random.randint(MAP_MAX_X) \n",
    "        posID_y = np.random.randint(MAP_MAX_Y)\n",
    "\n",
    "        request = \"map{},{},{},50,100\".format(mapID, posID_x, posID_y)\n",
    "        env.send_map_info(request)\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        #current_step = 0\n",
    "        env.reset()\n",
    "        obs = env.get_state()\n",
    "        for step in range(n_max_steps):\n",
    "            obs, reward, done, grads = play_one_step(env, obs, model, loss_fn)\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            #current_step += 1\n",
    "            if done:\n",
    "                break\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "        #all_n_steps.append(current_step)\n",
    "    return all_rewards, all_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_rate):\n",
    "    discounted = np.array(rewards)\n",
    "    for step in range(len(rewards) - 2, -1, -1):\n",
    "        discounted[step] += discounted[step + 1] * discount_rate\n",
    "    return discounted\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_rate)\n",
    "                              for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean) / reward_std\n",
    "            for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 150\n",
    "n_episodes_per_update = 10\n",
    "n_max_steps = 100\n",
    "discount_rate = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.01)\n",
    "loss_fn = keras.losses.categorical_crossentropy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Maps = CreateMaps()\n",
    "minerEnv = MinerEnv()\n",
    "minerEnv.start()\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    all_rewards, all_grads = play_multiple_episodes(\n",
    "        minerEnv, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
    "    total_rewards = sum(map(sum, all_rewards))\n",
    "    print(\"\\rIteration: {}, mean rewards: {:.1f}\".format(\n",
    "        iteration, total_rewards / n_episodes_per_update), end=\"\")\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n",
    "                                                       discount_rate)\n",
    "    all_mean_grads = []\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        mean_grads = tf.reduce_mean(\n",
    "            [final_reward * all_grads[episode_index][step][var_index]\n",
    "             for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "                 for step, final_reward in enumerate(final_rewards)], axis=0)\n",
    "        all_mean_grads.append(mean_grads)\n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Connected to server.\n",
    "Found: map0\n",
    "Found: map1\n",
    "Found: map2\n",
    "Found: map3\n",
    "Found: map4\n",
    "Reset game:  ['map3', '14', '7', '50', '100']\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "TypeError                                 Traceback (most recent call last)\n",
    "<ipython-input-106-d74c9a6d1318> in <module>\n",
    "      5 for iteration in range(n_iterations):\n",
    "      6     all_rewards, all_grads = play_multiple_episodes(\n",
    "----> 7         minerEnv, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
    "      8     total_rewards = sum(map(sum, all_rewards))\n",
    "      9     print(\"\\rIteration: {}, mean rewards: {:.1f}\".format(\n",
    "\n",
    "<ipython-input-102-5e8618d3fb3d> in play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn)\n",
    "     17         obs = env.get_state()\n",
    "     18         for step in range(n_max_steps):\n",
    "---> 19             obs, reward, done, grads = play_one_step(env, obs, model, loss_fn)\n",
    "     20             current_rewards.append(reward)\n",
    "     21             current_grads.append(grads)\n",
    "\n",
    "<ipython-input-82-e5b06cc2b076> in play_one_step(env, obs, model, loss_fn)\n",
    "      3         action_proba = model(obs[np.newaxis])\n",
    "      4         #action = (tf.random.uniform([1, 1]) > left_proba)\n",
    "----> 5         action = np.random.choice(range(N_ACTIONS), p=action_proba[0])\n",
    "      6         #y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n",
    "      7         y_target_pre = np.zeros((1, N_ACTIONS), dtype=np.float32)\n",
    "\n",
    "mtrand.pyx in numpy.random.mtrand.RandomState.choice()\n",
    "\n",
    "TypeError: object of type 'Tensor' has no len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`action_proba = model.predict(obs[np.newaxis])` instead of `action_proba = model(obs[np.newaxis])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=149, shape=(1, 6), dtype=float32, numpy=\n",
       "array([[0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
       "        0.16666667]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(np.zeros((1,198)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
       "        0.16666667]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.zeros((1,198)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        action_proba = model.predict(obs[np.newaxis])\n",
    "        #action = (tf.random.uniform([1, 1]) > left_proba)\n",
    "        action = np.random.choice(range(N_ACTIONS), p=action_proba[0])\n",
    "        #y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n",
    "        y_target_pre = np.zeros((1, N_ACTIONS), dtype=np.float32)\n",
    "        y_target_pre[0, action] = 1.\n",
    "        y_target = tf.constant(y_target_pre)\n",
    "        #y_target = tf.constant(y_target_pre, dtype=tf.float32)\n",
    "        # \"pretend for now that whatever action it takes is the right one\"\n",
    "        loss = tf.reduce_mean(loss_fn(y_target, action_proba))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    #obs, reward, done, info = env.step(int(action[0, 0].numpy()))\n",
    "    env.step(str(action))\n",
    "    obs = env.get_state()\n",
    "    reward = env.get_reward()\n",
    "    done = env.check_terminate()\n",
    "    return obs, reward, done, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, **NOOOOO**, replacing tensor by ndarray will generate another problem, because we are recording tensors in a tape."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Maps = CreateMaps()\n",
    "minerEnv = MinerEnv()\n",
    "minerEnv.start()\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    all_rewards, all_grads = play_multiple_episodes(\n",
    "        minerEnv, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
    "    total_rewards = sum(map(sum, all_rewards))\n",
    "    print(\"\\rIteration: {}, mean rewards: {:.1f}\".format(\n",
    "        iteration, total_rewards / n_episodes_per_update), end=\"\")\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n",
    "                                                       discount_rate)\n",
    "    all_mean_grads = []\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        mean_grads = tf.reduce_mean(\n",
    "            [final_reward * all_grads[episode_index][step][var_index]\n",
    "             for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "                 for step, final_reward in enumerate(final_rewards)], axis=0)\n",
    "        all_mean_grads.append(mean_grads)\n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Connected to server.\n",
    "Found: map0\n",
    "Found: map1\n",
    "Found: map2\n",
    "Found: map3\n",
    "Found: map4\n",
    "Iteration: 0, mean rewards: -12.4'50', '100']Reset game:  ['map2', '10', '7', '50', '100']Reset game:  ['map2', '9', '2', '50', '100']Reset game:  ['map3', '17', '7', '50', '100']Reset game:  ['map3', '7', '4', '50', '100']Reset game:  ['map4', '0', '8', '50', '100']Reset game:  ['map2', '0', '4', '50', '100']Reset game:  ['map4', '13', '6', '50', '100']Reset game:  ['map2', '3', '6', '50', '100']Reset game:  ['map0', '19', '8', '50', '100']\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "TypeError                                 Traceback (most recent call last)\n",
    "<ipython-input-108-d74c9a6d1318> in <module>\n",
    "     15         mean_grads = tf.reduce_mean(\n",
    "     16             [final_reward * all_grads[episode_index][step][var_index]\n",
    "---> 17              for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "     18                  for step, final_reward in enumerate(final_rewards)], axis=0)\n",
    "     19         all_mean_grads.append(mean_grads)\n",
    "\n",
    "<ipython-input-108-d74c9a6d1318> in <listcomp>(.0)\n",
    "     16             [final_reward * all_grads[episode_index][step][var_index]\n",
    "     17              for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "---> 18                  for step, final_reward in enumerate(final_rewards)], axis=0)\n",
    "     19         all_mean_grads.append(mean_grads)\n",
    "     20     optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n",
    "\n",
    "TypeError: unsupported operand type(s) for *: 'float' and 'NoneType'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tensorflow._api.v1.compat.v1.random' from '/home/leif/.virtualenvs/rlcomp2020/lib64/python3.6/site-packages/tensorflow/_api/v1/compat/v1/random/__init__.py'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        action_proba = model(obs[np.newaxis])\n",
    "        #action = (tf.random.uniform([1, 1]) > left_proba)\n",
    "        #with tf.Session() as sess:\n",
    "        #    action_proba_ndarray = sess.run(action_proba)\n",
    "        #action = np.random.choice(range(N_ACTIONS), p=action_proba_ndarray[0])\n",
    "        action = np.random.choice(range(N_ACTIONS), p=action_proba.numpy()[0])\n",
    "        #print(\"action_proba.numpy() = {}\".format(action_proba.numpy()))\n",
    "        #y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n",
    "        y_target_pre = np.zeros((1, N_ACTIONS), dtype=np.float32)\n",
    "        y_target_pre[0, action] = 1.\n",
    "        y_target = tf.constant(y_target_pre)\n",
    "        #y_target = tf.constant(y_target_pre, dtype=tf.float32)\n",
    "        # \"pretend for now that whatever action it takes is the right one\"\n",
    "        loss = tf.reduce_mean(loss_fn(y_target, action_proba))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    #obs, reward, done, info = env.step(int(action[0, 0].numpy()))\n",
    "    env.step(str(action))\n",
    "    obs = env.get_state()\n",
    "    reward = env.get_reward()\n",
    "    done = env.check_terminate()\n",
    "    return obs, reward, done, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_MAX_X = 21\n",
    "MAP_MAX_Y = 9\n",
    "\n",
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    all_n_steps = []\n",
    "    for episode in range(n_episodes):\n",
    "        mapID = np.random.randint(0, 5)\n",
    "        posID_x = np.random.randint(MAP_MAX_X) \n",
    "        posID_y = np.random.randint(MAP_MAX_Y)\n",
    "\n",
    "        request = \"map{},{},{},50,100\".format(mapID, posID_x, posID_y)\n",
    "        env.send_map_info(request)\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        current_step = 0\n",
    "        env.reset()\n",
    "        obs = env.get_state()\n",
    "        for step in range(n_max_steps):\n",
    "            obs, reward, done, grads = play_one_step(env, obs, model, loss_fn)\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            current_step += 1\n",
    "            if done:\n",
    "                break\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "        all_n_steps.append(current_step)\n",
    "    return all_rewards, all_grads, all_n_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Maps = CreateMaps()\n",
    "minerEnv = MinerEnv()\n",
    "minerEnv.start()\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    all_rewards, all_grads = play_multiple_episodes(\n",
    "        minerEnv, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
    "    total_rewards = sum(map(sum, all_rewards))\n",
    "    print(\"\\rIteration: {}, mean rewards: {:.1f}\".format(\n",
    "        iteration, total_rewards / n_episodes_per_update), end=\"\")\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n",
    "                                                       discount_rate)\n",
    "    all_mean_grads = []\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        mean_grads = tf.reduce_mean(\n",
    "            [final_reward * all_grads[episode_index][step][var_index]\n",
    "             for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "                 for step, final_reward in enumerate(final_rewards)], axis=0)\n",
    "        all_mean_grads.append(mean_grads)\n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to server.\n",
      "Found: map0\n",
      "Found: map1\n",
      "Found: map2\n",
      "Found: map3\n",
      "Found: map4\n",
      "WARNING:tensorflow:From /home/leif/.virtualenvs/rlcomp2020/lib64/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1220: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Iteration: 16, mean rewards: -80.0, mean steps: 100.0"
     ]
    }
   ],
   "source": [
    "Maps = CreateMaps()\n",
    "minerEnv = MinerEnv()\n",
    "minerEnv.start()\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    all_rewards, all_grads, all_n_steps = play_multiple_episodes(\n",
    "        minerEnv, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
    "    total_rewards = sum(map(sum, all_rewards))\n",
    "    total_n_steps = sum(all_n_steps)\n",
    "    print(\"\\rIteration: {}, mean rewards: {:.1f}, mean steps: {:.1f}\".format(\n",
    "        iteration, total_rewards / n_episodes_per_update, total_n_steps / n_episodes_per_update), end=\"\")\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n",
    "                                                       discount_rate)\n",
    "    all_mean_grads = []\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        mean_grads = tf.reduce_mean(\n",
    "            [final_reward * all_grads[episode_index][step][var_index]\n",
    "             for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "                 for step, final_reward in enumerate(final_rewards)], axis=0)\n",
    "        all_mean_grads.append(mean_grads)\n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n",
    "    if iteration > 100:\n",
    "        model.save(\"iter{:03d}.h5\".format(iteration))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.save(\"iter66.h5\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.predict(np.zeros((1,198)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Miner_Training_Colab_CodeSample.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
