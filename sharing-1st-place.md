Dear cÃ¡c Ä‘á»™i thi,

DÆ°á»›i Ä‘Ã¢y lÃ  bÃ i tá»•ng há»£p vá» cÃ¡c kÄ© thuáº­t trong viá»‡c cáº£i thiá»‡n model DQN, Ä‘Æ°á»£c chia sáº» bá»Ÿi VNPT_IC Team (Team Ä‘ang dáº«n Ä‘áº§u trÃªn báº£ng xáº¿p háº¡ng)

BTC mong ráº±ng bÃ i viáº¿t sáº½ giÃºp Ã­ch cho cÃ¡c team khÃ¡c trong viá»‡c training model.

 

ğŸŒˆ Double DQN (https://arxiv.org/pdf/1509.06461.pdf) : Thay vÃ¬ sá»­ dá»¥ng 1 máº¡ng neural chung Ä‘á»ƒ select action vÃ  tÃ­nh Q-value ta sáº½ sá»­ dá»¥ng 2 máº¡ng neural, 1 máº¡ng Ä‘á»ƒ select action vÃ  1 máº¡ng Ä‘á»ƒ tÃ­nh Q-value. 2 máº¡ng neural nÃ y chung 1 kiáº¿n trÃºc, chá»‰ khÃ¡c nhau vá» weight.

ğŸŒˆ Dueling Network (https://arxiv.org/pdf/1511.06581.pdf) : Output máº¡ng neural thay vÃ¬ chá»‰ Ä‘Æ°a ra giÃ¡ trá»‹ Q(s,a) thÃ¬ Ä‘Æ°a ra 2 giÃ¡ trá»‹ V(s) vÃ  A(s,a). Sau Ä‘Ã³ káº¿t há»£p 2 giÃ¡ trá»‹ nÃ y Ä‘á»ƒ tÃ­nh ra Q(s,a).

 

ğŸŒˆ Prioritized Experience Replay (https://arxiv.org/pdf/1511.05952.pdf) : 1 cáº£i tiáº¿n cho váº¥n Ä‘á» sampling replay. Vá»›i model DQN trÆ°á»›c Ä‘Ã¢y ta chá»‰ random ngáº«u nhiÃªn cÃ¡c replay tá»« buffer. Ã tÆ°á»Ÿng cá»§a PER lÃ  thay vÃ¬ láº¥y random ta sáº½ Æ°u tiÃªn láº¥y nhÆ°ng replay chÆ°a Ä‘Æ°á»£c sá»­ dá»¥ng vÃ  nhá»¯ng replay cÃ³ TD-error cao.

 

ğŸŒˆ N-Step Learning : Thay vÃ¬ sá»­ dá»¥ng 1 step Ä‘á»ƒ tÃ­nh Q target, giÃ¡ trá»‹ reward cá»§a n-step liÃªn tiáº¿p sáº½ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ­nh Q target.

 

ğŸŒˆ Distributed Agents: Thay vÃ¬ chá»‰ sá»­ dá»¥ng 1 actor Ä‘á»ƒ generate sample ta sáº½ dÃ¹ng multi actors. CÃ¡c actor nÃ y cháº¡y song song trÃªn nhiá»u luá»“ng cÃ¹ng lÃºc. KÄ© thuáº­t nÃ y khÃ´ng nhá»¯ng giÃºp viá»‡c huáº¥n luyá»‡n nhanh hÆ¡n mÃ  cÃ²n giÃºp tÄƒng Ä‘Ã¡ng ká»ƒ Ä‘á»™ chÃ­nh xÃ¡c. Vá»›i viá»‡c Ä‘áº·t cho má»—i actors 1 há»‡ sá»‘ epsilon khÃ¡c nhau ta cÃ²n cÃ³ thá»ƒ khiáº¿n mÃ´ hÃ¬nh khÃ¡m phÃ¡ ra nhiá»u nÆ°á»›c Ä‘i má»›i hÆ¡n. Háº§u háº¿t cÃ¡c kiáº¿n trÃºc DQN state-of-the-art hiá»‡n nay Ä‘á»u Ä‘ang lÃ  Distributed Agents (Ape-x, R2D2, NGU, Agent57, Muzero, â€¦.)

 

ğŸŒˆ Short term memory (https://openreview.net/pdf?id=r1lyTjAqYX): Vá»›i nhá»¯ng bÃ i toÃ¡n yáº¿u tá»‘ sequence Ä‘Ã³ng vai trÃ² quan trá»ng, nhÆ° lá»‹ch sá»­ cá»§a cÃ¡c state trÆ°á»›c cÃ³ áº£nh hÆ°á»Ÿng nhiá»u Ä‘áº¿n nhá»¯ng state sau thÃ¬ Ä‘Ã¢y lÃ  1 phÆ°Æ¡ng phÃ¡p khÃ¡ hiá»‡u quáº£.

 

ğŸŒˆ Muzero (https://arxiv.org/pdf/1911.08265.pdf) : ÄÃ¢y lÃ  1 mÃ´ hÃ¬nh cáº£i tiáº¿n tá»« AlphaZero. Náº¿u nhÆ° AlphaZero chá»‰ cÃ³ thá»ƒ Ã¡p dá»¥ng Ä‘Æ°á»£c trÃªn nhá»¯ng game cá» thÃ¬ giá» Ä‘Ã¢y Muzero cÃ³ Ä‘Ã£ cÃ³ thá»ƒ Ã¡p dá»¥ng thuáº­t toÃ¡n tÆ°Æ¡ng tá»± trÃªn táº¥t cáº£ cÃ¡c game khÃ¡c. Vá» thuáº­t toÃ¡n tÃ¬m kiáº¿m Monte carlo tree search thÃ¬ khÃ´ng cÃ³ gÃ¬ thay Ä‘á»•i so vá»›i AlphaZero. Äiá»ƒm khÃ¡c biáº¿t lÃ  Muzero cÃ³ thÃªm 2 máº¡ng neural, 1 máº¡ng Ä‘á»ƒ predict reward cá»§a mÃ´i trÆ°á»ng tráº£ vá», 1 máº¡ng Ä‘á»ƒ tÃ¡i hiá»‡n láº¡i tráº¡ng thÃ¡i cá»§a mÃ´i trÆ°á»ng á»Ÿ nhá»¯ng state tiáº¿p theo. Nhá» Ä‘Ã³ Muzero cÃ³ thá»ƒ Ã¡p dá»¥ng thuáº­t toÃ¡n tÃ¬m kiáº¿m cÃ¢y MTCC mÃ  khÃ´ng cáº§n pháº£i sá»­ dá»¥ng Ä‘áº¿n mÃ´i trÆ°á»ng tráº£ vá», nghÄ©a lÃ  mÃ´ hÃ¬nh sáº½ tá»± há»c ra má»™t game cá»§a riÃªng nÃ³. Muzero hiá»‡n Ä‘ang Ä‘áº¡t SOTA trÃªn háº§u háº¿t cÃ¡c game atari hiá»‡n nay.

 

ğŸŒˆ Agent 57 (https://arxiv.org/pdf/2003.13350.pdf) : ÄÃ¢y cÅ©ng lÃ  má»™t model khÃ¡ ná»•i tiáº¿ng, má»›i Ä‘Æ°á»£c Google Deepmind public gáº§n Ä‘Ã¢y, model Ä‘áº¡t káº¿t quáº£ trÃªn human performance trÃªn toÃ n bá»™ 57 games Atari. Model lÃ  sá»± káº¿t há»£p cá»§a ráº¥t nhiá»u kÄ© thuáº­t cáº£i tiáº¿n cá»§a DQN. Trong Ä‘Ã³ cáº£i tiáº¿n Ä‘Ã¡ng ká»ƒ nháº¥t lÃ  kÄ© thuáº­t â€œintrinsic motivation rewardsâ€ vÃ  â€œmeta-controllerâ€. 2 kÄ© thuáº­t nÃ y giÃºp model giáº£i quyáº¿t váº¥n Ä‘á» khÃ¡ nan giáº£i trong DQN lÃ  exploration (viá»‡c khÃ¡m phÃ¡ cÃ¡c nÆ°á»›c Ä‘i má»›i máº·c dÃ¹ cÃ³ thá»ƒ khÃ´ng tá»‘t báº±ng cÃ¡c nÆ°á»›c Ä‘i cÃ¹ng thá»i Ä‘iá»ƒm nhÆ°ng Ä‘em láº¡i reward tá»‘t hÆ¡n trong cÃ¡c nÆ°á»›c Ä‘i tÆ°Æ¡ng lai) vÃ  time horizon (nhá»¯ng bÃ i toÃ¡n mÃ  reward chá»‰ Ä‘áº¡t Ä‘Æ°á»£c sau ráº¥t nhiá»u state)

 

ChÃºc cÃ¡c Ä‘á»™i thi thÃ nh cÃ´ng!
