{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 100_000\n",
    "\n",
    "#collect_episodes_per_iteration = 5\n",
    "replay_buffer_capacity = 2000\n",
    "\n",
    "batch_size = 8\n",
    "collect_steps_per_iteration = 1\n",
    "lr_optimizer = 2.5e-4\n",
    "log_interval = 5\n",
    "n_eval_episodes = 10\n",
    "eval_interval = 10\n",
    "n_iterations_before_save = 5_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "#from tf_agents.environments import py_environment as pyenv, tf_py_environment, utils\n",
    "#from tf_agents.specs import array_spec \n",
    "from tf_agents.trajectories import time_step\n",
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "from tf_agents.networks.q_network import QNetwork\n",
    "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.eval.metric_utils import log_metrics\n",
    "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
    "from tf_agents.drivers.dynamic_episode_driver import DynamicEpisodeDriver\n",
    "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
    "from tf_agents.utils.common import function, Checkpointer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.policies import policy_saver\n",
    "\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.pardir))\n",
    "import constants02\n",
    "from constants02 import width, height, agent_state_id2str\n",
    "\n",
    "from miner_env import MinerEnv\n",
    "from tf_agents_miner_env import TFAgentsMiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sMitx5qSgJk1"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to server.\n",
      "Found: map10\n",
      "Found: map6\n",
      "Found: map3\n",
      "Found: map5\n",
      "Found: map9\n",
      "Found: map8\n",
      "Found: map1\n",
      "Found: map2\n",
      "Found: map4\n",
      "Found: map7\n",
      "Connected to server.\n",
      "Found: map10\n",
      "Found: map6\n",
      "Found: map3\n",
      "Found: map5\n",
      "Found: map9\n",
      "Found: map8\n",
      "Found: map1\n",
      "Found: map2\n",
      "Found: map4\n",
      "Found: map7\n"
     ]
    }
   ],
   "source": [
    "train_env = TFPyEnvironment(TFAgentsMiner())\n",
    "eval_env = TFPyEnvironment(TFAgentsMiner())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the Deep Q-Network\n",
    "preprocessing_layer = keras.layers.Lambda(\n",
    "    lambda obs: tf.cast(obs, np.float32)/255.)\n",
    "conv_layer_params = [(4,(3,3),1), (8,(3,3),1)]\n",
    "fc_layer_params = [128, 64, 32]\n",
    "\n",
    "q_net = QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    conv_layer_params=conv_layer_params,\n",
    "    fc_layer_params=fc_layer_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the DQN Agent\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "optimizer = keras.optimizers.RMSprop(lr=2.5e-4, rho=0.95, momentum=0.0, epsilon=0.00001, centered=True)\n",
    "\n",
    "epsilon_fn = keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=1.0, # initial ε\n",
    "    #decay_steps=380_000,\n",
    "    decay_steps=30_000,\n",
    "    end_learning_rate=0.01) # final ε\n",
    "\n",
    "agent = DqnAgent(train_env.time_step_spec(),\n",
    "                 train_env.action_spec(),\n",
    "                 q_network=q_net,\n",
    "                 optimizer=optimizer,\n",
    "                 target_update_period=50,\n",
    "                 td_errors_loss_fn=keras.losses.Huber(reduction=\"none\"),\n",
    "                 gamma=0.95, # discount factor\n",
    "                 train_step_counter=global_step,\n",
    "                 epsilon_greedy=lambda: epsilon_fn(global_step),\n",
    ")\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BwY7StuMkuV4"
   },
   "outputs": [],
   "source": [
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HE37-UCIrE69"
   },
   "outputs": [],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5gCcpXswVAxk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to server.\n",
      "Found: map10\n",
      "Found: map6\n",
      "Found: map3\n",
      "Found: map5\n",
      "Found: map9\n",
      "Found: map8\n",
      "Found: map1\n",
      "Found: map2\n",
      "Found: map4\n",
      "Found: map7\n"
     ]
    }
   ],
   "source": [
    "example_environment = TFPyEnvironment(TFAgentsMiner())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D4DHZtq3Ndis"
   },
   "outputs": [],
   "source": [
    "time_step = example_environment.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PRFqAUzpNaAW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([5], dtype=int32)>, state=(), info=())"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_policy.action(time_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "94rCXQtbUbXv"
   },
   "source": [
    "## Metrics and Evaluation\n",
    "\n",
    "The most common metric used to evaluate a policy is the average return. The return is the sum of rewards obtained while running a policy in an environment for an episode. Several episodes are run, creating an average return.\n",
    "\n",
    "The following function computes the average return of a policy, given the policy, environment, and a number of episodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bitzHo5_UbXy"
   },
   "outputs": [],
   "source": [
    "#@test {\"skip\": true}\n",
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n",
    "\n",
    "\n",
    "# See also the metrics module for standard implementations of different metrics.\n",
    "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_snCVvq5Z8lJ"
   },
   "source": [
    "Running this computation on the `random_policy` shows a baseline performance in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9bgU6Q6BZ8Bp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1150.66"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_avg_return(eval_env, random_policy, n_eval_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NLva6g2jdWgr"
   },
   "source": [
    "## Replay Buffer\n",
    "\n",
    "The replay buffer keeps track of data collected from the environment. This tutorial uses `tf_agents.replay_buffers.tf_uniform_replay_buffer.TFUniformReplayBuffer`, as it is the most common. \n",
    "\n",
    "The constructor requires the specs for the data it will be collecting. This is available from the agent using the `collect_data_spec` method. The batch size and maximum buffer length are also required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vX2zGUWJGWAl"
   },
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_capacity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZGNTDJpZs4NN"
   },
   "source": [
    "For most agents, `collect_data_spec` is a named tuple called `Trajectory`, containing the specs for observations, actions, rewards, and other items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rVD5nQ9ZGo8_"
   },
   "source": [
    "## Data Collection\n",
    "\n",
    "Now execute the random policy in the environment for a few steps, recording the data in the replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wr1KSAEGG4h9"
   },
   "outputs": [],
   "source": [
    "#@test {\"skip\": true}\n",
    "def collect_step(environment, policy, buffer):\n",
    "  time_step = environment.current_time_step()\n",
    "  action_step = policy.action(time_step)\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "  # Add trajectory to the replay buffer\n",
    "  buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "  for _ in range(steps):\n",
    "    collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, steps=100)\n",
    "\n",
    "# This loop is so common in RL, that we provide standard implementations. \n",
    "# For more details see the drivers module.\n",
    "# https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84z5pQJdoKxo"
   },
   "source": [
    "The replay buffer is now a collection of Trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ba7bilizt_qW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (Trajectory(step_type=(8, 2), observation=(8, 2, 9, 21, 2), action=(8, 2), policy_info=(), next_step_type=(8, 2), reward=(8, 2), discount=(8, 2)), BufferInfo(ids=(8, 2), probabilities=(8,))), types: (Trajectory(step_type=tf.int32, observation=tf.float32, action=tf.int32, policy_info=(), next_step_type=tf.int32, reward=tf.float32, discount=tf.float32), BufferInfo(ids=tf.int64, probabilities=tf.float32))>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K13AST-2ppOq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7f0da04647f0>\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(dataset)\n",
    "\n",
    "print(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Trajectory(step_type=<tf.Tensor: shape=(8, 2), dtype=int32, numpy=\n",
       " array([[1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 2],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 2],\n",
       "        [1, 1]], dtype=int32)>, observation=<tf.Tensor: shape=(8, 2, 9, 21, 2), dtype=float32, numpy=\n",
       " array([[[[[ 2.00e+02,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [ 4.50e+02,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00]],\n",
       " \n",
       "          [[-1.00e+01,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          [[-5.10e+00,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [ 1.50e+02,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  5.00e+01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 4.50e+02,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          [[-5.10e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00]],\n",
       " \n",
       "          [[-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [ 4.00e+02,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00]]],\n",
       " \n",
       " \n",
       "         [[[ 2.00e+02,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [ 4.50e+02,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00]],\n",
       " \n",
       "          [[-1.00e+01,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-2.01e+01,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          [[-5.10e+00,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [ 1.50e+02,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 4.50e+02,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          [[-5.10e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00]],\n",
       " \n",
       "          [[-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [ 4.00e+02,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00]]]],\n",
       " \n",
       " \n",
       " \n",
       "        [[[[-1.00e+00,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          [[-5.10e+00,  0.00e+00],\n",
       "           [ 5.00e+02,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [ 3.50e+02,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00]],\n",
       " \n",
       "          [[-2.00e+01,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 3.50e+02,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [ 2.00e+02,  0.00e+00]],\n",
       " \n",
       "          [[-1.00e+00,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           ...,\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [ 3.00e+02,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          [[-1.00e+00,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [ 4.00e+02,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [ 8.00e+02,  0.00e+00]]],\n",
       " \n",
       " \n",
       "         [[[-1.00e+00,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          [[-5.10e+00,  0.00e+00],\n",
       "           [ 5.00e+02,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [ 3.50e+02,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00]],\n",
       " \n",
       "          [[-2.00e+01,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 3.50e+02,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [ 2.00e+02,  0.00e+00]],\n",
       " \n",
       "          [[-1.00e+00,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           ...,\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [ 3.00e+02,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          [[-1.00e+00,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [ 4.00e+02,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [ 8.00e+02,  0.00e+00]]]],\n",
       " \n",
       " \n",
       " \n",
       "        [[[[ 3.00e+02,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          [[ 3.50e+02,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          [[-2.00e+01,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [ 2.00e+02,  0.00e+00],\n",
       "           ...,\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-5.10e+00,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          [[-2.00e+01,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [ 2.00e+02,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          [[ 8.00e+02,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]]],\n",
       " \n",
       " \n",
       "         [[[ 2.50e+02,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          [[ 3.50e+02,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          [[-2.00e+01,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [ 1.50e+02,  0.00e+00],\n",
       "           ...,\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-5.10e+00,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          [[-2.00e+01,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [ 2.00e+02,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          [[ 8.00e+02,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]]]],\n",
       " \n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       " \n",
       "        [[[[-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           ...,\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [ 8.00e+02,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00]],\n",
       " \n",
       "          [[ 1.00e+02,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00]],\n",
       " \n",
       "          [[-2.00e+01,  0.00e+00],\n",
       "           [ 7.00e+02,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.00e+00,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00]],\n",
       " \n",
       "          [[-1.00e+00,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           ...,\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          [[-2.00e+01,  0.00e+00],\n",
       "           [ 5.00e+02,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [ 5.00e+01,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]]],\n",
       " \n",
       " \n",
       "         [[[-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           ...,\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [ 8.00e+02,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00]],\n",
       " \n",
       "          [[ 1.00e+02,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00]],\n",
       " \n",
       "          [[-2.00e+01,  0.00e+00],\n",
       "           [ 7.00e+02,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.00e+00,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00]],\n",
       " \n",
       "          [[-1.00e+00,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           ...,\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          [[-2.00e+01,  0.00e+00],\n",
       "           [ 5.00e+02,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [ 5.00e+01,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]]]],\n",
       " \n",
       " \n",
       " \n",
       "        [[[[ 2.50e+02,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          [[ 3.50e+02,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          [[-2.00e+01,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [ 1.00e+02,  0.00e+00],\n",
       "           ...,\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-5.10e+00,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          [[-2.00e+01,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [ 2.00e+02,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          [[ 8.00e+02,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]]],\n",
       " \n",
       " \n",
       "         [[[ 2.00e+02,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          [[ 3.50e+02,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          [[-2.00e+01,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           [ 1.00e+02,  0.00e+00],\n",
       "           ...,\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-5.10e+00,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          [[-2.00e+01,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [ 2.00e+02,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]],\n",
       " \n",
       "          [[ 8.00e+02,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00]]]],\n",
       " \n",
       " \n",
       " \n",
       "        [[[[ 4.50e+02,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           ...,\n",
       "           [ 1.50e+02,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [ 3.50e+02,  0.00e+00]],\n",
       " \n",
       "          [[-1.00e+01,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [ 5.00e+01,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00]],\n",
       " \n",
       "          [[-1.00e+01,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [ 2.00e+02,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [ 5.00e+01,  0.00e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.00e+01,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00]],\n",
       " \n",
       "          [[-1.00e+00,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [ 2.00e+02,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00]],\n",
       " \n",
       "          [[ 1.20e+03,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [ 1.50e+03,  0.00e+00],\n",
       "           [ 5.00e+01,  0.00e+00]]],\n",
       " \n",
       " \n",
       "         [[[ 4.50e+02,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           ...,\n",
       "           [ 1.50e+02,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [ 3.50e+02,  0.00e+00]],\n",
       " \n",
       "          [[-1.00e+01,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [ 5.00e+01,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00]],\n",
       " \n",
       "          [[-1.00e+01,  0.00e+00],\n",
       "           [-1.00e+01,  0.00e+00],\n",
       "           [ 2.00e+02,  0.00e+00],\n",
       "           ...,\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [-1.00e+00,  0.00e+00],\n",
       "           [ 5.00e+01,  0.00e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.00e+01,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00]],\n",
       " \n",
       "          [[-1.00e+00,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [ 2.00e+02,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00]],\n",
       " \n",
       "          [[ 1.20e+03,  0.00e+00],\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [-2.00e+01,  0.00e+00],\n",
       "           ...,\n",
       "           [-5.10e+00,  0.00e+00],\n",
       "           [ 1.50e+03,  0.00e+00],\n",
       "           [ 5.00e+01,  0.00e+00]]]]], dtype=float32)>, action=<tf.Tensor: shape=(8, 2), dtype=int32, numpy=\n",
       " array([[3, 2],\n",
       "        [4, 4],\n",
       "        [4, 2],\n",
       "        [3, 2],\n",
       "        [1, 1],\n",
       "        [5, 3],\n",
       "        [2, 0],\n",
       "        [4, 4]], dtype=int32)>, policy_info=(), next_step_type=<tf.Tensor: shape=(8, 2), dtype=int32, numpy=\n",
       " array([[1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [2, 2],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [2, 1],\n",
       "        [1, 1]], dtype=int32)>, reward=<tf.Tensor: shape=(8, 2), dtype=float32, numpy=\n",
       " array([[    0.,     0.],\n",
       "        [ -199.,  -199.],\n",
       "        [    1.,     0.],\n",
       "        [-1000., -1000.],\n",
       "        [    0.,   -19.],\n",
       "        [  -99.,     0.],\n",
       "        [-1000.,    51.],\n",
       "        [    1.,     1.]], dtype=float32)>, discount=<tf.Tensor: shape=(8, 2), dtype=float32, numpy=\n",
       " array([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 0.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 1.]], dtype=float32)>),\n",
       " BufferInfo(ids=<tf.Tensor: shape=(8, 2), dtype=int64, numpy=\n",
       " array([[78, 79],\n",
       "        [49, 50],\n",
       "        [36, 37],\n",
       "        [27, 28],\n",
       "        [66, 67],\n",
       "        [25, 26],\n",
       "        [38, 39],\n",
       "        [74, 75]])>, probabilities=<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       " array([0.01010101, 0.01010101, 0.01010101, 0.01010101, 0.01010101,\n",
       "        0.01010101, 0.01010101, 0.01010101], dtype=float32)>))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hBc9lj9VWWtZ"
   },
   "source": [
    "## Training the agent\n",
    "\n",
    "Two things must happen during the training loop:\n",
    "\n",
    "-   collect data from the environment\n",
    "-   use that data to train the agent's neural network(s)\n",
    "\n",
    "This example also periodicially evaluates the policy and prints the current score.\n",
    "\n",
    "The following will take ~5 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "now_str = now.strftime(\"%Y%m%d-%H%M\")\n",
    "#script_name = __file__.split('.')[0]\n",
    "script_name = \"debug_empty_buffer\"\n",
    "save_dir = os.path.join(\"models\", script_name)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "#logging.basicConfig(filename=os.path.join(save_path, f\"log-{now_str}.txt\"),level=logging.DEBUG)\n",
    "checkpoint_dir = os.path.join(save_dir, 'checkpoint')\n",
    "train_checkpointer = Checkpointer(\n",
    "    ckpt_dir=checkpoint_dir,\n",
    "    max_to_keep=500,\n",
    "    agent=agent,\n",
    "    policy=agent.policy,\n",
    "    replay_buffer=replay_buffer,\n",
    "    global_step=global_step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0pTbJ3PeyF-u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =       5: loss = 104.20884704589844\n",
      "step =      10: loss = 167.33189392089844\n",
      "step =      10: Average Return = -766.1099853515625\n",
      "step =      15: loss = 35.94205093383789\n",
      "step =      20: loss = 50.38216018676758\n",
      "step =      20: Average Return = -1025.469970703125\n",
      "step =      25: loss = 59.618499755859375\n",
      "step =      30: loss = 120.24795532226562\n",
      "step =      30: Average Return = -814.9000244140625\n",
      "step =      35: loss = 17.072147369384766\n",
      "step =      40: loss = 52.44712829589844\n",
      "step =      40: Average Return = -4278.09033203125\n",
      "step =      45: loss = 98.28364562988281\n",
      "step =      50: loss = 121.4784927368164\n",
      "step =      50: Average Return = -3427.150390625\n",
      "step =      55: loss = 116.77301025390625\n",
      "step =      60: loss = 273.4272155761719\n",
      "step =      60: Average Return = -1424.159912109375\n",
      "step =      65: loss = 116.74124908447266\n",
      "step =      70: loss = 41.36418914794922\n",
      "step =      70: Average Return = -984.1399536132812\n",
      "step =      75: loss = 3.6193490028381348\n",
      "step =      80: loss = 227.09568786621094\n",
      "step =      80: Average Return = -10263.73046875\n",
      "step =      85: loss = 193.94400024414062\n",
      "step =      90: loss = 52.62685775756836\n",
      "step =      90: Average Return = -3171.81005859375\n",
      "step =      95: loss = 31.39678955078125\n",
      "step =     100: loss = 11.102779388427734\n",
      "step =     100: Average Return = -841.0\n",
      "step =     105: loss = 227.1240692138672\n",
      "step =     110: loss = 59.985843658447266\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-22b4c8905205>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mavg_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_avg_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_eval_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'step = {0:7d}: Average Return = {1}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_return\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mreturns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_return\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-b5398f26c9aa>\u001b[0m in \u001b[0;36mcompute_avg_return\u001b[0;34m(environment, policy, num_episodes)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_last\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0maction_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m       \u001b[0mtime_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m       \u001b[0mepisode_return\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtotal_return\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mepisode_return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/rlcomp2020-tf2.2.0/lib/python3.6/site-packages/tf_agents/environments/tf_environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    230\u001b[0m           \u001b[0mcorresponding\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mobservation_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \"\"\"\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/rlcomp2020-tf2.2.0/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/rlcomp2020-tf2.2.0/lib/python3.6/site-packages/tf_agents/environments/tf_py_environment.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    317\u001b[0m           \u001b[0mflat_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_time_step_dtypes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m           name='step_py_func')\n\u001b[0m\u001b[1;32m    320\u001b[0m       \u001b[0mstep_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m       \u001b[0mflat_observations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/rlcomp2020-tf2.2.0/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\u001b[0m in \u001b[0;36mnumpy_function\u001b[0;34m(func, inp, Tout, name)\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0mSingle\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mwhich\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mcomputes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m   \"\"\"\n\u001b[0;32m--> 628\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mpy_func_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstateful\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/rlcomp2020-tf2.2.0/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\u001b[0m in \u001b[0;36mpy_func_common\u001b[0;34m(func, inp, Tout, stateful, name)\u001b[0m\n\u001b[1;32m    515\u001b[0m   \"\"\"\n\u001b[1;32m    516\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/rlcomp2020-tf2.2.0/lib/python3.6/site-packages/tf_agents/environments/tf_py_environment.py\u001b[0m in \u001b[0;36m_isolated_step_py\u001b[0;34m(*flattened_actions)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_isolated_step_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mflattened_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_step_py\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mflattened_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/rlcomp2020-tf2.2.0/lib/python3.6/site-packages/tf_agents/environments/tf_py_environment.py\u001b[0m in \u001b[0;36m_execute\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/rlcomp2020-tf2.2.0/lib/python3.6/site-packages/tf_agents/environments/tf_py_environment.py\u001b[0m in \u001b[0;36m_step_py\u001b[0;34m(*flattened_actions)\u001b[0m\n\u001b[1;32m    296\u001b[0m         packed = tf.nest.pack_sequence_as(\n\u001b[1;32m    297\u001b[0m             structure=self.action_spec(), flat_sequence=flattened_actions)\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_time_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_time_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/rlcomp2020-tf2.2.0/lib/python3.6/site-packages/tf_agents/environments/py_environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    172\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_time_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_time_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/rlcomp2020-tf2.2.0/lib/python3.6/site-packages/tf_agents/environments/batched_py_environment.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    149\u001b[0m       \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbatch_nested_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m       \u001b[0mtime_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_envs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_nested_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0munstacked_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munstack_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/rlcomp2020-tf2.2.0/lib/python3.6/site-packages/tf_agents/utils/nest_utils.py\u001b[0m in \u001b[0;36mbatch_nested_array\u001b[0;34m(nested_array)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbatch_nested_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnested_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/rlcomp2020-tf2.2.0/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[1;32m    617\u001b[0m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m       expand_composites=expand_composites)\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/rlcomp2020-tf2.2.0/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mpack_sequence_as\u001b[0;34m(structure, flat_sequence, expand_composites)\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msortable\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m   \"\"\"\n\u001b[0;32m--> 552\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_pack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "try:\n",
    "  %%time\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, n_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  for _ in range(collect_steps_per_iteration):\n",
    "    collect_step(train_env, agent.collect_policy, replay_buffer)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0:7d}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(eval_env, agent.policy, n_eval_episodes)\n",
    "    print('step = {0:7d}: Average Return = {1}'.format(step, avg_return))\n",
    "    returns.append(avg_return)\n",
    "    if step > n_iterations_before_save:\n",
    "      train_checkpointer.save(global_step)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "agent.train(experience)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "LossInfo(loss=<tf.Tensor: shape=(), dtype=float32, numpy=57.73095>, extra=DqnLossInfo(td_loss=<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
    "array([57.73095, 57.73095, 57.73095, 57.73095, 57.73095, 57.73095,\n",
    "       57.73095, 57.73095], dtype=float32)>, td_error=<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
    "array([   3.5509195,   -5.9019737,   14.058611 ,   61.152996 ,\n",
    "       -185.8588   ,   -4.75216  , -189.3686   ,   -1.2035675],\n",
    "      dtype=float32)>))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "68jNcA_TiJDq"
   },
   "source": [
    "## Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aO-LWCdbbOIC"
   },
   "source": [
    "### Plots\n",
    "\n",
    "Use `matplotlib.pyplot` to chart how the policy improved during training.\n",
    "\n",
    "One iteration of `Cartpole-v0` consists of 200 time steps. The environment gives a reward of `+1` for each step the pole stays up, so the maximum return for one episode is 200. The charts shows the return increasing towards that maximum each time it is evaluated during training. (It may be a little unstable and not increase monotonically each time.)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NxtL1mbOYCVO"
   },
   "source": [
    "#@test {\"skip\": true}\n",
    "\n",
    "iterations = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylim(top=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M7-XpPP99Cy7"
   },
   "source": [
    "### Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9pGfGxSH32gn"
   },
   "source": [
    "Charts are nice. But more exciting is seeing an agent actually performing a task in an environment. \n",
    "\n",
    "First, create a function to embed videos in the notebook."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ULaGr8pvOKbl"
   },
   "source": [
    "def embed_mp4(filename):\n",
    "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
    "  video = open(filename,'rb').read()\n",
    "  b64 = base64.b64encode(video)\n",
    "  tag = '''\n",
    "  <video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "  </video>'''.format(b64.decode())\n",
    "\n",
    "  return IPython.display.HTML(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9c_PH-pX4Pr5"
   },
   "source": [
    "Now iterate through a few episodes of the Cartpole game with the agent. The underlying Python environment (the one \"inside\" the TensorFlow environment wrapper) provides a `render()` method, which outputs an image of the environment state. These can be collected into a video."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "owOVWB158NlF"
   },
   "source": [
    "def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\n",
    "  filename = filename + \".mp4\"\n",
    "  with imageio.get_writer(filename, fps=fps) as video:\n",
    "    for _ in range(num_episodes):\n",
    "      time_step = eval_env.reset()\n",
    "      video.append_data(eval_py_env.render())\n",
    "      while not time_step.is_last():\n",
    "        action_step = policy.action(time_step)\n",
    "        time_step = eval_env.step(action_step.action)\n",
    "        video.append_data(eval_py_env.render())\n",
    "  return embed_mp4(filename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "create_policy_eval_video(agent.policy, \"trained-agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "povaAOcZygLw"
   },
   "source": [
    "For fun, compare the trained agent (above) to an agent moving randomly. (It does not do as well.)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pJZIdC37yNH4"
   },
   "source": [
    "create_policy_eval_video(random_policy, \"random-agent\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DQN Tutorial.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
