{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 100_000\n",
    "\n",
    "#collect_episodes_per_iteration = 5\n",
    "replay_buffer_capacity = 2000\n",
    "\n",
    "batch_size = 8\n",
    "collect_steps_per_iteration = 1\n",
    "lr_optimizer = 2.5e-4\n",
    "log_interval = 5\n",
    "n_eval_episodes = 10\n",
    "eval_interval = 10\n",
    "n_iterations_before_save = 5_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "#from tf_agents.environments import py_environment as pyenv, tf_py_environment, utils\n",
    "#from tf_agents.specs import array_spec \n",
    "from tf_agents.trajectories import time_step\n",
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "from tf_agents.networks.q_network import QNetwork\n",
    "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.eval.metric_utils import log_metrics\n",
    "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
    "from tf_agents.drivers.dynamic_episode_driver import DynamicEpisodeDriver\n",
    "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
    "from tf_agents.utils.common import function, Checkpointer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.policies import policy_saver\n",
    "\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.pardir))\n",
    "import constants02\n",
    "from constants02 import width, height, agent_state_id2str\n",
    "\n",
    "from miner_env import MinerEnv\n",
    "from tf_agents_miner_env import TFAgentsMiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sMitx5qSgJk1"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to server.\n",
      "Found: map10\n",
      "Found: map6\n",
      "Found: map3\n",
      "Found: map5\n",
      "Found: map9\n",
      "Found: map8\n",
      "Found: map1\n",
      "Found: map2\n",
      "Found: map4\n",
      "Found: map7\n",
      "Connected to server.\n",
      "Found: map10\n",
      "Found: map6\n",
      "Found: map3\n",
      "Found: map5\n",
      "Found: map9\n",
      "Found: map8\n",
      "Found: map1\n",
      "Found: map2\n",
      "Found: map4\n",
      "Found: map7\n"
     ]
    }
   ],
   "source": [
    "train_env = TFPyEnvironment(TFAgentsMiner())\n",
    "eval_env = TFPyEnvironment(TFAgentsMiner(debug=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the Deep Q-Network\n",
    "preprocessing_layer = keras.layers.Lambda(\n",
    "    lambda obs: tf.cast(obs, np.float32)/255.)\n",
    "conv_layer_params = [(4,(3,3),1), (8,(3,3),1)]\n",
    "fc_layer_params = [128, 64, 32]\n",
    "\n",
    "q_net = QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    conv_layer_params=conv_layer_params,\n",
    "    fc_layer_params=fc_layer_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the DQN Agent\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "optimizer = keras.optimizers.RMSprop(lr=2.5e-4, rho=0.95, momentum=0.0, epsilon=0.00001, centered=True)\n",
    "\n",
    "epsilon_fn = keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=1.0, # initial ε\n",
    "    #decay_steps=380_000,\n",
    "    decay_steps=30_000,\n",
    "    end_learning_rate=0.01) # final ε\n",
    "\n",
    "agent = DqnAgent(train_env.time_step_spec(),\n",
    "                 train_env.action_spec(),\n",
    "                 q_network=q_net,\n",
    "                 optimizer=optimizer,\n",
    "                 target_update_period=50,\n",
    "                 td_errors_loss_fn=keras.losses.Huber(reduction=\"none\"),\n",
    "                 gamma=0.95, # discount factor\n",
    "                 train_step_counter=global_step,\n",
    "                 epsilon_greedy=lambda: epsilon_fn(global_step),\n",
    ")\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BwY7StuMkuV4"
   },
   "outputs": [],
   "source": [
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HE37-UCIrE69"
   },
   "outputs": [],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5gCcpXswVAxk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to server.\n",
      "Found: map10\n",
      "Found: map6\n",
      "Found: map3\n",
      "Found: map5\n",
      "Found: map9\n",
      "Found: map8\n",
      "Found: map1\n",
      "Found: map2\n",
      "Found: map4\n",
      "Found: map7\n"
     ]
    }
   ],
   "source": [
    "example_environment = TFPyEnvironment(TFAgentsMiner())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D4DHZtq3Ndis"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 9, 21, 2), dtype=float32, numpy=\n",
       "array([[[[200. ,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [250. ,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [-20. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [500. ,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [450. ,   0. ],\n",
       "         [ -5.1,   0. ]],\n",
       "\n",
       "        [[-10. ,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [-20. ,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [-20. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [200. ,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [150. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [-20. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [ -1. ,   0. ]],\n",
       "\n",
       "        [[ -5.1,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [-20. ,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [-20. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [-20. ,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [450. ,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [150. ,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [ -1. ,   0. ]],\n",
       "\n",
       "        [[300. ,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [300. ,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [300. ,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [ -1. ,   0. ]],\n",
       "\n",
       "        [[ -5.1,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [-20. ,   0. ],\n",
       "         [350. ,   0. ],\n",
       "         [-20. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [350. ,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [300. ,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [250. ,   0. ]],\n",
       "\n",
       "        [[ -5.1,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [-20. ,   0. ],\n",
       "         [-20. ,   0. ],\n",
       "         [-20. ,   0. ],\n",
       "         [-20. ,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [400. ,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -1. ,   0. ]],\n",
       "\n",
       "        [[450. ,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [-20. ,   0. ],\n",
       "         [400. ,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -1. ,   0. ]],\n",
       "\n",
       "        [[ -5.1,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [-20. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [-20. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [-20. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [250. ,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [-20. ,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [-10. ,   0. ]],\n",
       "\n",
       "        [[ -1. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [-20. ,   0. ],\n",
       "         [200. ,   0. ],\n",
       "         [-20. ,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [500. ,   0. ],\n",
       "         [ -5.1,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [-20. ,   0. ],\n",
       "         [200. ,   0. ],\n",
       "         [-20. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [ -1. ,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [-10. ,   0. ],\n",
       "         [-20. ,   0. ],\n",
       "         [400. ,   0. ],\n",
       "         [-20. ,   0. ],\n",
       "         [-10. ,  50. ]]]], dtype=float32)>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step = example_environment.reset()\n",
    "time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PRFqAUzpNaAW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([4], dtype=int32)>, state=(), info=())"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_policy.action(time_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "94rCXQtbUbXv"
   },
   "source": [
    "## Metrics and Evaluation\n",
    "\n",
    "The most common metric used to evaluate a policy is the average return. The return is the sum of rewards obtained while running a policy in an environment for an episode. Several episodes are run, creating an average return.\n",
    "\n",
    "The following function computes the average return of a policy, given the policy, environment, and a number of episodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bitzHo5_UbXy"
   },
   "outputs": [],
   "source": [
    "#@test {\"skip\": true}\n",
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  #time_step = environment.reset()\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "    #print(f\"gold {environment.state.score}\")\n",
    "    \n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n",
    "\n",
    "\n",
    "# See also the metrics module for standard implementations of different metrics.\n",
    "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_snCVvq5Z8lJ"
   },
   "source": [
    "Running this computation on the `random_policy` shows a baseline performance in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9bgU6Q6BZ8Bp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map1,2,2,50,100\n",
      "gold    0\n",
      "map5,20,2,50,100\n",
      "map4,0,7,50,100\n",
      "gold    0\n",
      "map2,11,2,50,100\n",
      "map1,14,7,50,100\n",
      "gold    0\n",
      "map2,16,8,50,100\n",
      "map3,12,7,50,100\n",
      "gold    0\n",
      "map3,4,0,50,100\n",
      "map2,2,2,50,100\n",
      "gold    0\n",
      "map1,12,1,50,100\n",
      "map4,14,8,50,100\n",
      "gold    0\n",
      "map2,9,5,50,100\n",
      "map2,17,2,50,100\n",
      "gold   13\n",
      "map3,0,2,50,100\n",
      "map5,13,2,50,100\n",
      "gold    0\n",
      "map4,13,4,50,100\n",
      "map5,18,1,50,100\n",
      "gold    0\n",
      "map5,4,2,50,100\n",
      "map5,18,4,50,100\n",
      "gold    0\n",
      "map5,16,0,50,100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1353.1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#eval_env.reset()\n",
    "compute_avg_return(eval_env, random_policy, n_eval_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NLva6g2jdWgr"
   },
   "source": [
    "## Replay Buffer\n",
    "\n",
    "The replay buffer keeps track of data collected from the environment. This tutorial uses `tf_agents.replay_buffers.tf_uniform_replay_buffer.TFUniformReplayBuffer`, as it is the most common. \n",
    "\n",
    "The constructor requires the specs for the data it will be collecting. This is available from the agent using the `collect_data_spec` method. The batch size and maximum buffer length are also required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vX2zGUWJGWAl"
   },
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_capacity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZGNTDJpZs4NN"
   },
   "source": [
    "For most agents, `collect_data_spec` is a named tuple called `Trajectory`, containing the specs for observations, actions, rewards, and other items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rVD5nQ9ZGo8_"
   },
   "source": [
    "## Data Collection\n",
    "\n",
    "Now execute the random policy in the environment for a few steps, recording the data in the replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wr1KSAEGG4h9"
   },
   "outputs": [],
   "source": [
    "#@test {\"skip\": true}\n",
    "def collect_step(environment, policy, buffer):\n",
    "  time_step = environment.current_time_step()\n",
    "  action_step = policy.action(time_step)\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "  # Add trajectory to the replay buffer\n",
    "  buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "  for _ in range(steps):\n",
    "    collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, steps=100)\n",
    "\n",
    "# This loop is so common in RL, that we provide standard implementations. \n",
    "# For more details see the drivers module.\n",
    "# https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84z5pQJdoKxo"
   },
   "source": [
    "The replay buffer is now a collection of Trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ba7bilizt_qW"
   },
   "outputs": [],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K13AST-2ppOq"
   },
   "outputs": [],
   "source": [
    "iterator = iter(dataset)\n",
    "\n",
    "print(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hBc9lj9VWWtZ"
   },
   "source": [
    "## Training the agent\n",
    "\n",
    "Two things must happen during the training loop:\n",
    "\n",
    "-   collect data from the environment\n",
    "-   use that data to train the agent's neural network(s)\n",
    "\n",
    "This example also periodicially evaluates the policy and prints the current score.\n",
    "\n",
    "The following will take ~5 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "now_str = now.strftime(\"%Y%m%d-%H%M\")\n",
    "#script_name = __file__.split('.')[0]\n",
    "script_name = \"debug_empty_buffer\"\n",
    "save_dir = os.path.join(\"models\", script_name)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "#logging.basicConfig(filename=os.path.join(save_path, f\"log-{now_str}.txt\"),level=logging.DEBUG)\n",
    "checkpoint_dir = os.path.join(save_dir, 'checkpoint')\n",
    "train_checkpointer = Checkpointer(\n",
    "    ckpt_dir=checkpoint_dir,\n",
    "    max_to_keep=500,\n",
    "    agent=agent,\n",
    "    policy=agent.policy,\n",
    "    replay_buffer=replay_buffer,\n",
    "    global_step=global_step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0pTbJ3PeyF-u"
   },
   "outputs": [],
   "source": [
    "#@test {\"skip\": true}\n",
    "try:\n",
    "  %%time\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, n_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  for _ in range(collect_steps_per_iteration):\n",
    "    collect_step(train_env, agent.collect_policy, replay_buffer)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "  #train_env.log_info()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    #print('step = {0:7d}    loss = {1:7.2f}'.format(step, train_loss))\n",
    "    pass\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(eval_env, agent.policy, n_eval_episodes)\n",
    "    print('step = {0:7d}    AvgR = {1:7.2f}'.format(step, avg_return))\n",
    "    returns.append(avg_return)\n",
    "    if step > n_iterations_before_save:\n",
    "      train_checkpointer.save(global_step)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "agent.train(experience)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "LossInfo(loss=<tf.Tensor: shape=(), dtype=float32, numpy=57.73095>, extra=DqnLossInfo(td_loss=<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
    "array([57.73095, 57.73095, 57.73095, 57.73095, 57.73095, 57.73095,\n",
    "       57.73095, 57.73095], dtype=float32)>, td_error=<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
    "array([   3.5509195,   -5.9019737,   14.058611 ,   61.152996 ,\n",
    "       -185.8588   ,   -4.75216  , -189.3686   ,   -1.2035675],\n",
    "      dtype=float32)>))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "68jNcA_TiJDq"
   },
   "source": [
    "## Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aO-LWCdbbOIC"
   },
   "source": [
    "### Plots\n",
    "\n",
    "Use `matplotlib.pyplot` to chart how the policy improved during training.\n",
    "\n",
    "One iteration of `Cartpole-v0` consists of 200 time steps. The environment gives a reward of `+1` for each step the pole stays up, so the maximum return for one episode is 200. The charts shows the return increasing towards that maximum each time it is evaluated during training. (It may be a little unstable and not increase monotonically each time.)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NxtL1mbOYCVO"
   },
   "source": [
    "#@test {\"skip\": true}\n",
    "\n",
    "iterations = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylim(top=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M7-XpPP99Cy7"
   },
   "source": [
    "### Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9pGfGxSH32gn"
   },
   "source": [
    "Charts are nice. But more exciting is seeing an agent actually performing a task in an environment. \n",
    "\n",
    "First, create a function to embed videos in the notebook."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ULaGr8pvOKbl"
   },
   "source": [
    "def embed_mp4(filename):\n",
    "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
    "  video = open(filename,'rb').read()\n",
    "  b64 = base64.b64encode(video)\n",
    "  tag = '''\n",
    "  <video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "  </video>'''.format(b64.decode())\n",
    "\n",
    "  return IPython.display.HTML(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9c_PH-pX4Pr5"
   },
   "source": [
    "Now iterate through a few episodes of the Cartpole game with the agent. The underlying Python environment (the one \"inside\" the TensorFlow environment wrapper) provides a `render()` method, which outputs an image of the environment state. These can be collected into a video."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "owOVWB158NlF"
   },
   "source": [
    "def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\n",
    "  filename = filename + \".mp4\"\n",
    "  with imageio.get_writer(filename, fps=fps) as video:\n",
    "    for _ in range(num_episodes):\n",
    "      time_step = eval_env.reset()\n",
    "      video.append_data(eval_py_env.render())\n",
    "      while not time_step.is_last():\n",
    "        action_step = policy.action(time_step)\n",
    "        time_step = eval_env.step(action_step.action)\n",
    "        video.append_data(eval_py_env.render())\n",
    "  return embed_mp4(filename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "create_policy_eval_video(agent.policy, \"trained-agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "povaAOcZygLw"
   },
   "source": [
    "For fun, compare the trained agent (above) to an agent moving randomly. (It does not do as well.)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pJZIdC37yNH4"
   },
   "source": [
    "create_policy_eval_video(random_policy, \"random-agent\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DQN Tutorial.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
