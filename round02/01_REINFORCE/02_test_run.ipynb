{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "#from tf_agents.environments import py_environment as pyenv, tf_py_environment, utils\n",
    "#from tf_agents.specs import array_spec \n",
    "from tf_agents.trajectories import time_step\n",
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "from tf_agents.networks.q_network import QNetwork\n",
    "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.eval.metric_utils import log_metrics\n",
    "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
    "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
    "from tf_agents.utils.common import function, Checkpointer\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.agents.reinforce import reinforce_agent\n",
    "from tf_agents.policies import policy_saver\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.trajectories import trajectory\n",
    "\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.pardir))\n",
    "import constants02\n",
    "from constants02 import width, height, agent_state_id2str\n",
    "\n",
    "from miner_env import MinerEnv\n",
    "from tf_agents_miner_env import TFAgentsMiner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 250\n",
    "collect_episodes_per_iteration = 2\n",
    "replay_buffer_capacity = 2000\n",
    "\n",
    "lr_optimizer = 2.5e-4\n",
    "log_interval = 25\n",
    "n_eval_episodes = 10\n",
    "eval_interval = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to server.\n",
      "Found: map3\n",
      "Found: map5\n",
      "Found: map1\n",
      "Found: map2\n",
      "Found: map4\n",
      "Connected to server.\n",
      "Found: map3\n",
      "Found: map5\n",
      "Found: map1\n",
      "Found: map2\n",
      "Found: map4\n"
     ]
    }
   ],
   "source": [
    "train_env = TFPyEnvironment(TFAgentsMiner())\n",
    "eval_env = TFPyEnvironment(TFAgentsMiner())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer_params = [(4,(3,3),1), (8,(3,3),1)]\n",
    "fc_layer_params = [128, 64, 32]\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    conv_layer_params=conv_layer_params,\n",
    "    fc_layer_params=fc_layer_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=lr_optimizer)\n",
    "#optimizer = keras.optimizers.RMSprop(lr=lr_optimizer, rho=0.95, momentum=0.0, epsilon=0.00001, centered=True)\n",
    "\n",
    "train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "tf_agent = reinforce_agent.ReinforceAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    actor_network=actor_net,\n",
    "    optimizer=optimizer,\n",
    "    normalize_returns=True,\n",
    "    train_step_counter=train_step_counter,\n",
    ")\n",
    "\n",
    "tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = tf_agent.policy\n",
    "collect_policy = tf_agent.collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(?1)** Do we need to set `train_env.batch_size` in our case for the `miner_env`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, n_episodes=10):\n",
    "    total_return = 0.0\n",
    "    for _ in range(n_episodes):\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / n_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=tf_agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_capacity,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trajectory(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), observation=BoundedTensorSpec(shape=(9, 21, 2), dtype=tf.float32, name='observation', minimum=array(-3.4028235e+38, dtype=float32), maximum=array(3.4028235e+38, dtype=float32)), action=BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(5, dtype=int32)), policy_info=(), next_step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_agent.collect_data_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_episode(environment, policy, n_episodes):\n",
    "    episode_counter = 0\n",
    "    environment.reset()\n",
    "\n",
    "    while episode_counter < n_episodes:\n",
    "        time_step = environment.current_time_step()\n",
    "        action_step = policy.action(time_step)\n",
    "        next_time_step = environment.step(action_step.action)\n",
    "        traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "        # Add trajectory to the replay buffer\n",
    "        replay_buffer.add_batch(traj)\n",
    "\n",
    "        if traj.is_boundary():\n",
    "            episode_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =    25: loss = -0.0015380163677036762\n",
      "step =    50: loss = -0.001120886066928506\n",
      "step =    50: Average Return = -520.4000244140625\n",
      "step =    75: loss = -3.3229513064725325e-05\n",
      "step =   100: loss = -3.819010453298688e-05\n",
      "step =   100: Average Return = -528.9000244140625\n",
      "step =   125: loss = -0.000583361485041678\n",
      "step =   150: loss = -0.000344128260621801\n",
      "step =   150: Average Return = -497.70001220703125\n",
      "step =   175: loss = -0.0024425156880170107\n",
      "step =   200: loss = -1.3129118997312617e-05\n",
      "step =   200: Average Return = -484.20001220703125\n",
      "step =   225: loss = -0.014528408646583557\n",
      "step =   250: loss = -2.2877455194247887e-05\n",
      "step =   250: Average Return = -548.9000244140625\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    %%time\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "tf_agent.train = common.function(tf_agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "tf_agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, tf_agent.policy, n_eval_episodes)\n",
    "#avg_return = tf_metrics.AverageReturnMetric(eval_env, tf_agent.policy, n_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "\n",
    "    # Collect a few episodes using collect_policy and save to the replay buffer.\n",
    "    collect_episode(\n",
    "            train_env, tf_agent.collect_policy, collect_episodes_per_iteration)\n",
    "\n",
    "    # Use data from the buffer and update the agent's network.\n",
    "    #dataset = replay_buffer.as_dataset(single_deterministic_pass=True)\n",
    "    #iterator = iter(dataset)\n",
    "    #experience, buffer_info = next(iterator)\n",
    "    #experience = next(iterator)\n",
    "\n",
    "    #experience = replay_buffer.as_dataset(single_deterministic_pass=True)\n",
    "    \n",
    "    experience = replay_buffer.gather_all()\n",
    "    train_loss = tf_agent.train(experience)\n",
    "    replay_buffer.clear()\n",
    "\n",
    "    step = tf_agent.train_step_counter.numpy()\n",
    "    #print(\"step = {}\".format(step))\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0:5d}  loss = {1}'.format(step, train_loss.loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, tf_agent.policy, n_eval_episodes)\n",
    "        #avg_return = tf_metrics.AverageReturnMetric(eval_env, tf_agent.policy, n_eval_episodes)\n",
    "        print('step = {0:5d}  Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
