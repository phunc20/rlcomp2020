{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "#from tf_agents.environments import py_environment as pyenv, tf_py_environment, utils\n",
    "#from tf_agents.specs import array_spec \n",
    "from tf_agents.trajectories import time_step\n",
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "from tf_agents.networks.q_network import QNetwork\n",
    "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.eval.metric_utils import log_metrics\n",
    "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
    "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
    "from tf_agents.utils.common import function, Checkpointer\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.agents.reinforce import reinforce_agent\n",
    "from tf_agents.policies import policy_saver\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.trajectories import trajectory\n",
    "\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.pardir))\n",
    "import constants02\n",
    "from constants02 import width, height, agent_state_id2str\n",
    "\n",
    "from miner_env import MinerEnv\n",
    "from tf_agents_miner_env import TFAgentsMiner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 10_000\n",
    "collect_episodes_per_iteration = 2\n",
    "replay_buffer_capacity = 2000\n",
    "\n",
    "lr_optimizer = 2.5e-4\n",
    "log_interval = 25\n",
    "n_eval_episodes = 20\n",
    "eval_interval = 100\n",
    "n_iterations_skipped = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to server.\n",
      "Found: map3\n",
      "Found: map5\n",
      "Found: map1\n",
      "Found: map2\n",
      "Found: map4\n",
      "Connected to server.\n",
      "Found: map3\n",
      "Found: map5\n",
      "Found: map1\n",
      "Found: map2\n",
      "Found: map4\n"
     ]
    }
   ],
   "source": [
    "train_env = TFPyEnvironment(TFAgentsMiner())\n",
    "eval_env = TFPyEnvironment(TFAgentsMiner())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer_params = [(4,(3,3),1), (8,(3,3),1)]\n",
    "fc_layer_params = [128, 64, 32]\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    conv_layer_params=conv_layer_params,\n",
    "    fc_layer_params=fc_layer_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=lr_optimizer)\n",
    "#optimizer = keras.optimizers.RMSprop(lr=lr_optimizer, rho=0.95, momentum=0.0, epsilon=0.00001, centered=True)\n",
    "\n",
    "#train_step_counter = tf.compat.v2.Variable(0)\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "tf_agent = reinforce_agent.ReinforceAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    actor_network=actor_net,\n",
    "    optimizer=optimizer,\n",
    "    normalize_returns=True,\n",
    "    #train_step_counter=train_step_counter,\n",
    "    train_step_counter=global_step,\n",
    ")\n",
    "\n",
    "tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_step.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = tf_agent.policy\n",
    "collect_policy = tf_agent.collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(?1)** Do we need to set `train_env.batch_size` in our case for the `miner_env`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, n_episodes=10):\n",
    "    total_return = 0.0\n",
    "    for _ in range(n_episodes):\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / n_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=tf_agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_capacity,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trajectory(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), observation=BoundedTensorSpec(shape=(9, 21, 2), dtype=tf.float32, name='observation', minimum=array(-3.4028235e+38, dtype=float32), maximum=array(3.4028235e+38, dtype=float32)), action=BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(5, dtype=int32)), policy_info=(), next_step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_agent.collect_data_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_episode(environment, policy, n_episodes):\n",
    "    episode_counter = 0\n",
    "    environment.reset()\n",
    "\n",
    "    while episode_counter < n_episodes:\n",
    "        time_step = environment.current_time_step()\n",
    "        action_step = policy.action(time_step)\n",
    "        next_time_step = environment.step(action_step.action)\n",
    "        traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "        # Add trajectory to the replay buffer\n",
    "        replay_buffer.add_batch(traj)\n",
    "\n",
    "        if traj.is_boundary():\n",
    "            episode_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =  2800  Average Return = -782.0999755859375\n",
      "step =  2900  Average Return = -751.6500244140625\n",
      "step =  3000  Average Return = -831.9500122070312\n",
      "step =  3100  Average Return = -862.7000122070312\n",
      "step =  3200  Average Return = -833.2999877929688\n",
      "step =  3300  Average Return = -793.5999755859375\n",
      "step =  3400  Average Return = -825.4500122070312\n",
      "step =  3500  Average Return = -796.2999877929688\n",
      "step =  3600  Average Return = -785.7000122070312\n",
      "step =  3700  Average Return = -804.4000244140625\n",
      "step =  3800  Average Return = -843.5499877929688\n",
      "step =  3900  Average Return = -817.4000244140625\n",
      "step =  4000  Average Return = -841.0499877929688\n",
      "step =  4100  Average Return = -817.0999755859375\n",
      "step =  4200  Average Return = -817.5999755859375\n",
      "step =  4300  Average Return = -821.5\n",
      "step =  4400  Average Return = -798.4500122070312\n",
      "step =  4500  Average Return = -892.7999877929688\n",
      "step =  4600  Average Return = -887.1500244140625\n",
      "step =  4700  Average Return = -877.7999877929688\n",
      "step =  4800  Average Return = -824.7000122070312\n",
      "step =  4900  Average Return = -807.3499755859375\n",
      "step =  5000  Average Return = -812.5499877929688\n",
      "step =  5100  Average Return = -855.7999877929688\n",
      "step =  5200  Average Return = -788.0\n",
      "step =  5300  Average Return = -811.2000122070312\n",
      "step =  5400  Average Return = -843.5499877929688\n",
      "step =  5500  Average Return = -884.2000122070312\n",
      "step =  5600  Average Return = -793.5\n",
      "step =  5700  Average Return = -833.0\n",
      "step =  5800  Average Return = -835.1500244140625\n",
      "step =  5900  Average Return = -816.2000122070312\n",
      "step =  6000  Average Return = -796.5999755859375\n",
      "step =  6100  Average Return = -812.3499755859375\n",
      "step =  6200  Average Return = -701.0\n",
      "step =  6300  Average Return = -830.1500244140625\n",
      "step =  6400  Average Return = -836.7999877929688\n",
      "step =  6500  Average Return = -885.9000244140625\n",
      "step =  6600  Average Return = -865.7000122070312\n",
      "step =  6700  Average Return = -782.5499877929688\n",
      "step =  6800  Average Return = -852.2000122070312\n",
      "step =  6900  Average Return = -845.2000122070312\n",
      "step =  7000  Average Return = -861.0499877929688\n",
      "step =  7100  Average Return = -793.6500244140625\n",
      "step =  7200  Average Return = -837.0\n",
      "step =  7300  Average Return = -780.4000244140625\n",
      "step =  7400  Average Return = -874.6500244140625\n",
      "step =  7500  Average Return = -891.0999755859375\n",
      "step =  7600  Average Return = -770.4000244140625\n",
      "step =  7700  Average Return = -813.9500122070312\n",
      "step =  7800  Average Return = -833.5999755859375\n",
      "step =  7900  Average Return = -821.2000122070312\n",
      "step =  8000  Average Return = -812.5999755859375\n",
      "step =  8100  Average Return = -802.4500122070312\n",
      "step =  8200  Average Return = -821.0499877929688\n",
      "step =  8300  Average Return = -844.8499755859375\n",
      "step =  8400  Average Return = -816.5\n",
      "step =  8500  Average Return = -779.9000244140625\n",
      "step =  8600  Average Return = -820.2999877929688\n",
      "step =  8700  Average Return = -888.2999877929688\n",
      "step =  8800  Average Return = -798.0999755859375\n",
      "step =  8900  Average Return = -813.9500122070312\n",
      "step =  9000  Average Return = -771.0999755859375\n",
      "step =  9100  Average Return = -805.9000244140625\n",
      "step =  9200  Average Return = -791.7999877929688\n",
      "step =  9300  Average Return = -794.5499877929688\n",
      "step =  9400  Average Return = -807.5\n",
      "step =  9500  Average Return = -793.5999755859375\n",
      "step =  9600  Average Return = -815.0499877929688\n",
      "step =  9700  Average Return = -819.2000122070312\n",
      "step =  9800  Average Return = -772.0499877929688\n",
      "step =  9900  Average Return = -806.75\n",
      "step = 10000  Average Return = -776.25\n",
      "step = 10100  Average Return = -811.2999877929688\n",
      "step = 10200  Average Return = -805.9000244140625\n",
      "step = 10300  Average Return = -796.5\n",
      "step = 10400  Average Return = -792.75\n",
      "step = 10500  Average Return = -827.0499877929688\n",
      "step = 10600  Average Return = -834.5\n",
      "step = 10700  Average Return = -787.0999755859375\n",
      "step = 10800  Average Return = -762.5999755859375\n",
      "step = 10900  Average Return = -798.0\n",
      "step = 11000  Average Return = -825.0999755859375\n",
      "step = 11100  Average Return = -860.75\n",
      "step = 11200  Average Return = -806.9000244140625\n",
      "step = 11300  Average Return = -827.75\n",
      "step = 11400  Average Return = -852.4500122070312\n",
      "step = 11500  Average Return = -810.5999755859375\n",
      "step = 11600  Average Return = -828.5\n",
      "step = 11700  Average Return = -787.9500122070312\n",
      "step = 11800  Average Return = -842.4000244140625\n",
      "step = 11900  Average Return = -786.9500122070312\n",
      "step = 12000  Average Return = -758.5\n",
      "step = 12100  Average Return = -854.5\n",
      "step = 12200  Average Return = -819.75\n",
      "step = 12300  Average Return = -805.75\n",
      "step = 12400  Average Return = -760.7000122070312\n",
      "step = 12500  Average Return = -835.2000122070312\n",
      "step = 12600  Average Return = -816.4500122070312\n",
      "step = 12700  Average Return = -822.9000244140625\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    %%time\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "tf_agent.train = common.function(tf_agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "tf_agent.train_step_counter.assign(0)  # Can we get rid of this?\n",
    "global_step.assign(0)\n",
    "\n",
    "\n",
    "#now = datetime.datetime.now()\n",
    "#now_str = now.strftime(\"%Y%m%d-%H%M\")\n",
    "#script_name = __file__.split('.')[0]\n",
    "script_name = \"03_checkpointer\"\n",
    "save_dir = os.path.join(\"models\", script_name)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "#logging.basicConfig(filename=os.path.join(save_path, f\"log-{now_str}.txt\"),level=logging.DEBUG)\n",
    "checkpoint_dir = os.path.join(save_dir, 'checkpoint')\n",
    "\n",
    "train_checkpointer = Checkpointer(\n",
    "    ckpt_dir=checkpoint_dir,\n",
    "    max_to_keep=500,\n",
    "    agent=tf_agent,\n",
    "    policy=tf_agent.policy,\n",
    "    replay_buffer=replay_buffer,\n",
    "    global_step=global_step\n",
    ")\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, tf_agent.policy, n_eval_episodes)\n",
    "#avg_return = tf_metrics.AverageReturnMetric(eval_env, tf_agent.policy, n_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "\n",
    "    # Collect a few episodes using collect_policy and save to the replay buffer.\n",
    "    collect_episode(train_env, tf_agent.collect_policy, collect_episodes_per_iteration)\n",
    "\n",
    "    # Use data from the buffer and update the agent's network.\n",
    "    #dataset = replay_buffer.as_dataset(single_deterministic_pass=True)\n",
    "    #iterator = iter(dataset)\n",
    "    #experience, buffer_info = next(iterator)\n",
    "    #experience = next(iterator)\n",
    "\n",
    "    #experience = replay_buffer.as_dataset(single_deterministic_pass=True)\n",
    "    \n",
    "    experience = replay_buffer.gather_all()\n",
    "    train_loss = tf_agent.train(experience)\n",
    "    replay_buffer.clear()\n",
    "\n",
    "    step = tf_agent.train_step_counter.numpy()\n",
    "    #print(\"step = {}\".format(step))\n",
    "\n",
    "    #if step % log_interval == 0:\n",
    "    #    print('step = {0:5d}  loss = {1}'.format(step, train_loss.loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, tf_agent.policy, n_eval_episodes)\n",
    "        #avg_return = tf_metrics.AverageReturnMetric(eval_env, tf_agent.policy, n_eval_episodes)\n",
    "        print('step = {0:5d}  Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)\n",
    "        if step > n_iterations_skipped:\n",
    "            train_checkpointer.save(global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=12750>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_step.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=12750>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_agent.train_step_counter.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-d8a442e3fef8>:1: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-d8a442e3fef8>:1: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=12750>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_step.initialized_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12750"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_step.numpy()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "global_step.initial_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_step.assign(0)\n",
    "global_step.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
